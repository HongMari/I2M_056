# new056.py (EA ìë¦¬ì•µì»¤ + ìš”ëª©í‘œ 3ìë¦¬ íŒë‹¨ê¸°ì¤€ í†µí•© + ê·¼ê±° ë³´ê°•)

import os
import re
import json
import html
import urllib.parse
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
from bs4 import BeautifulSoup
from pathlib import Path

import requests
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ISBN â†’ KDC ì¶”ì²œ", page_icon="ğŸ“š", layout="centered")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜/ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "gpt-4o-mini"
ALADIN_LOOKUP_URL = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx"
OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"
NLK_SEARCH_API = "https://www.nl.go.kr/NL/search/openApi/search.do"
NLK_SEOJI_API  = "https://www.nl.go.kr/seoji/SearchApi.do"
KDC_OUTLINE_PDF = "/mnt/data/kdc ìš”ëª©í‘œ.pdf"  # ì—…ë¡œë“œëœ ìš”ëª©í‘œ

with st.expander("í™˜ê²½ì„¤ì • ë””ë²„ê·¸", expanded=True):
    st.write("ğŸ“ ì•± í´ë”:", Path(__file__).resolve().parent.as_posix())
    st.write("ğŸ” secrets.toml ì¡´ì¬?:", (Path(__file__).resolve().parent / ".streamlit" / "secrets.toml").exists())
    st.write("ğŸ”‘ st.secrets í‚¤ë“¤:", list(st.secrets.keys()))
    st.write("api_keys ë‚´ìš©:", dict(st.secrets.get("api_keys", {})))
    st.write("âœ… openai_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("openai_key")))
    st.write("âœ… aladin_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("aladin_key")))
    st.write("âœ… nlk_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("nlk_key")))
    st.write("ğŸ“„ ìš”ëª©í‘œ PDF ì¡´ì¬?:", Path(KDC_OUTLINE_PDF).exists())

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; KDCFetcher/1.0; +https://example.local)"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ secrets.toml ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_secret(*path, default=""):
    try:
        v = st.secrets
        for p in path:
            v = v[p]
        return v
    except Exception:
        return default

OPENAI_API_KEY = (_get_secret("api_keys", "openai_key") or os.environ.get("OPENAI_API_KEY", ""))
ALADIN_TTBKEY  = (_get_secret("api_keys", "aladin_key")  or os.environ.get("ALADIN_TTBKEY", ""))
NLK_API_KEY    = (_get_secret("api_keys", "nlk_key")     or os.environ.get("NLK_API_KEY", ""))
MODEL = DEFAULT_MODEL

@dataclass
class BookInfo:
    title: str = ""
    author: str = ""
    pub_date: str = ""
    publisher: str = ""
    isbn13: str = ""
    category: str = ""
    description: str = ""
    toc: str = ""
    extra: Dict[str, Any] = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def first_match_number(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"\b([0-9]{1,3}(?:\.[0-9]+)?)\b", text)
    return m.group(1) if m else None

def strip_tags(html_text: str) -> str:
    return re.sub(r"<[^>]+>", " ", html_text)

def normalize_isbn13(isbn: str) -> str:
    s = re.sub(r"[^0-9Xx]", "", isbn or "")
    return s[-13:] if len(s) >= 13 else s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš”ëª©í‘œ(3ìë¦¬) ë¡œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def load_kdc_outline3() -> Dict[str, Dict[str, Any]]:
    """
    ë°˜í™˜: {"000":{"label":"ì´ë¥˜","terms":["ì´ë¥˜"]}, "010":{"label":"ë„ì„œí•™ ì„œì§€í•™","terms":[...]}, ...}
    PDFê°€ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ì¶•ì•½ ì‚¬ì „ìœ¼ë¡œ í´ë°±.
    """
    outline: Dict[str, Dict[str, Any]] = {}
    try:
        import PyPDF2
        p = Path(KDC_OUTLINE_PDF)
        if not p.exists():
            raise FileNotFoundError("KDC outline PDF not found")
        reader = PyPDF2.PdfReader(open(p, "rb"))
        text = ""
        for pg in reader.pages:
            try:
                text += pg.extract_text() + "\n"
            except Exception:
                pass
        # íŒ¨í„´: "í•œê¸€ ë° ê³µë°±ë“¤ + 3ìë¦¬ìˆ«ì"
        for m in re.findall(r"([ê°€-í£A-Za-zÂ·\s]+?)(\d{3})", text):
            label = clean_text(m[0])
            code3 = m[1]
            if not label:
                continue
            # ê°™ì€ ì½”ë“œê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ë©´ label ëˆ„ì 
            rec = outline.get(code3, {"label": "", "terms": set()})
            # ëŒ€í‘œ ë¼ë²¨ì€ ê°€ì¥ ì§§ì€(í•µì‹¬) í…ìŠ¤íŠ¸ë¡œ
            if not rec["label"] or len(label) < len(rec["label"]):
                rec["label"] = label
            # ê²€ìƒ‰ìš© terms: ë¼ë²¨ì„ ê³µë°±ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ì¶”ê°€
            for t in label.split():
                if len(t) >= 2:
                    rec["terms"].add(t)
            outline[code3] = rec
        # set â†’ listë¡œ ë³€í™˜
        for k, v in outline.items():
            v["terms"] = sorted(list(v["terms"]))
        # ìµœì €í•œ: 000~900 ë¼ë²¨ì´ ì—†ìœ¼ë©´ ì¶•ì•½ê°’ ë³´ì¶©
        if "000" not in outline:
            outline["000"] = {"label":"ì´ë¥˜","terms":["ì´ë¥˜","ì§€ì‹","ë¬¸í—Œì •ë³´"]}
        if "100" not in outline:
            outline["100"] = {"label":"ì² í•™","terms":["ì² í•™","ìœ¤ë¦¬","ë…¼ë¦¬","ì‹¬ë¦¬"]}
        if "200" not in outline:
            outline["200"] = {"label":"ì¢…êµ","terms":["ì¢…êµ","ë¶ˆêµ","ê¸°ë…êµ","ì´ìŠ¬ëŒ"]}
        if "300" not in outline:
            outline["300"] = {"label":"ì‚¬íšŒê³¼í•™","terms":["ê²½ì œ","ê²½ì˜","ì •ì¹˜","ë²•","êµìœ¡","ì‚¬íšŒ"]}
        if "400" not in outline:
            outline["400"] = {"label":"ìì—°ê³¼í•™","terms":["ìˆ˜í•™","ë¬¼ë¦¬","í™”í•™","ìƒë¬¼","ì²œë¬¸"]}
        if "500" not in outline:
            outline["500"] = {"label":"ê¸°ìˆ ê³¼í•™","terms":["ì˜í•™","ê³µí•™","ê±´ì¶•","ë†ì—…","ì „ê¸°"]}
        if "600" not in outline:
            outline["600"] = {"label":"ì˜ˆìˆ ","terms":["ë¯¸ìˆ ","ìŒì•…","ë””ìì¸","ì‚¬ì§„","ì˜í™”"]}
        if "700" not in outline:
            outline["700"] = {"label":"ì–¸ì–´","terms":["ì–¸ì–´","ë¬¸ë²•","ì‚¬ì „","ì‘ë¬¸","íšŒí™”"]}
        if "800" not in outline:
            outline["800"] = {"label":"ë¬¸í•™","terms":["ë¬¸í•™","ì†Œì„¤","ì‹œ","í¬ê³¡","ìˆ˜í•„"]}
        if "900" not in outline:
            outline["900"] = {"label":"ì—­ì‚¬","terms":["ì—­ì‚¬","ì§€ë¦¬","ì „ê¸°","ì„¸ê³„ì‚¬","í•œêµ­ì‚¬"]}
        return outline
    except Exception as e:
        st.info(f"ìš”ëª©í‘œ PDF íŒŒì‹± ì‹¤íŒ¨ â†’ ì¶•ì•½ ì‚¬ì „ ì‚¬ìš©: {e}")
        return {
            "000":{"label":"ì´ë¥˜","terms":["ì´ë¥˜","ë¬¸í—Œì •ë³´","ë°±ê³¼ì‚¬ì „"]},
            "010":{"label":"ë„ì„œí•™ ì„œì§€í•™","terms":["ë„ì„œí•™","ì„œì§€í•™"]},
            "020":{"label":"ë¬¸í—Œì •ë³´í•™","terms":["ë¬¸í—Œì •ë³´í•™"]},
            "100":{"label":"ì² í•™","terms":["ì² í•™","ìœ¤ë¦¬","ë…¼ë¦¬","ì‹¬ë¦¬"]},
            "300":{"label":"ì‚¬íšŒê³¼í•™","terms":["ê²½ì œ","ê²½ì˜","ì •ì¹˜","ë²•","êµìœ¡","ì‚¬íšŒ"]},
            "400":{"label":"ìì—°ê³¼í•™","terms":["ìˆ˜í•™","ë¬¼ë¦¬","í™”í•™","ìƒë¬¼","ì²œë¬¸"]},
            "500":{"label":"ê¸°ìˆ ê³¼í•™","terms":["ì˜í•™","ê³µí•™","ê±´ì¶•","ë†ì—…","ì „ê¸°"]},
            "600":{"label":"ì˜ˆìˆ ","terms":["ë¯¸ìˆ ","ìŒì•…","ë””ìì¸","ì‚¬ì§„","ì˜í™”"]},
            "700":{"label":"ì–¸ì–´","terms":["ì–¸ì–´","ë¬¸ë²•","ì‚¬ì „","ì‘ë¬¸","íšŒí™”"]},
            "800":{"label":"ë¬¸í•™","terms":["ë¬¸í•™","ì†Œì„¤","ì‹œ","í¬ê³¡","ìˆ˜í•„"]},
            "900":{"label":"ì—­ì‚¬","terms":["ì—­ì‚¬","ì§€ë¦¬","ì „ê¸°","ì„¸ê³„ì‚¬","í•œêµ­ì‚¬"]},
        }

KDC3 = load_kdc_outline3()

def outline_slice_by_ryu(ryu: Optional[str]) -> Dict[str, Dict[str, Any]]:
    """ë°±ì˜ ìë¦¬ê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ 100ëŒ€(ì˜ˆ: '8' â†’ 800~899)ë§Œ ì˜ë¼ì„œ ë°˜í™˜."""
    if not (ryu and ryu.isdigit()):
        return KDC3
    prefix = ryu
    return {k:v for k,v in KDC3.items() if k.startswith(prefix)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EA_ADD_CODE ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_ea_add_code_last3(isbn13: str, key: str) -> Optional[str]:
    if not key:
        st.info("NLK_API_KEYê°€ ì—†ì–´ EA_ADD_CODE ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    # 1) ì„œì§€(ISBN) API
    try:
        p1 = {"cert_key": key, "result_style": "json", "page_no": 1, "page_size": 5, "isbn": isbn13}
        r1 = requests.get(NLK_SEOJI_API, params=p1, headers=HEADERS, timeout=10)
        r1.raise_for_status()
        d1 = r1.json()
        docs = d1.get("docs") if isinstance(d1, dict) else None
        if isinstance(docs, list) and docs:
            ea = docs[0].get("EA_ADD_CODE") or docs[0].get("ea_add_code")
            if ea:
                m = re.search(r"(\d{3})$", str(ea))
                if m:
                    last3 = m.group(1); st.success(f"(ì„œì§€API) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                    return last3
    except Exception as e:
        st.info(f"ì„œì§€API ì‹¤íŒ¨ â†’ ì¼ë°˜ê²€ìƒ‰ ë°±ì—…: {e}")
    # 2) ì¼ë°˜ê²€ìƒ‰ ë°±ì—…
    try:
        p2 = {"key": key, "srchTarget": "total", "kwd": isbn13, "pageNum": 1, "pageSize": 1, "apiType": "json"}
        r2 = requests.get(NLK_SEARCH_API, params=p2, headers=HEADERS, timeout=10)
        r2.raise_for_status()
        d2 = r2.json()
        result = d2.get("result") if isinstance(d2, dict) else None
        if isinstance(result, list):
            result = result[0] if result else {}
        recs = None
        if isinstance(result, dict):
            recs = result.get("recordList") or result.get("recordlist") or result.get("records") or result.get("record")
        if isinstance(recs, dict):
            recs = [recs]
        if isinstance(recs, list) and recs:
            rec0 = recs[0]
            if isinstance(rec0, list) and rec0:
                rec0 = rec0[0]
            if isinstance(rec0, dict):
                ea = rec0.get("EA_ADD_CODE") or rec0.get("ea_add_code")
                if ea:
                    m = re.search(r"(\d{3})$", str(ea))
                    if m:
                        last3 = m.group(1); st.success(f"(ì¼ë°˜ê²€ìƒ‰) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                        return last3
        st.warning("NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: ì‘ë‹µ êµ¬ì¡° ë¯¸ì¼ì¹˜")
        return None
    except Exception as e:
        st.warning(f"NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìë¦¬ë³„ ì•µì»¤ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_anchor_from_last3(last3: Optional[str]) -> Dict[str, Optional[str]]:
    anchors = {"hundreds": None, "tens": None, "units": None, "pattern": "x-x-x"}
    if not (last3 and len(last3) == 3 and last3.isdigit()):
        return anchors
    h, t, u = last3[0], last3[1], last3[2]
    anchors["hundreds"] = h if int(h) > 0 else None
    anchors["tens"]     = t if int(t) > 0 else None
    anchors["units"]    = u if int(u) > 0 else None
    anchors["pattern"]  = f"{anchors['hundreds'] or 'x'}-{anchors['tens'] or 'x'}-{anchors['units'] or 'x'}"
    return anchors

def anchor_clause_for_prompt(anc: Dict[str, Optional[str]], outline_hint: str) -> str:
    rules = []
    if anc.get("hundreds"): rules.append(f"ë°±ì˜ ìë¦¬ëŠ” {anc['hundreds']}")
    if anc.get("tens"):     rules.append(f"ì‹­ì˜ ìë¦¬ëŠ” {anc['tens']}")
    if anc.get("units"):    rules.append(f"ì¼ì˜ ìë¦¬ëŠ” {anc['units']}")
    base = ""
    if rules:
        mask = anc.get("pattern", "x-x-x").replace("-", "")
        examples = [mask.replace("x", d) for d in ["0","1","2"]]
        base = (" ë°˜ë“œì‹œ ë‹¤ìŒ ìë¦¿ìˆ˜ ì œì•½ì„ ì§€ì¼œë¼: " + ", ".join(rules) +
                f". ì¦‰, ë¶„ë¥˜ë²ˆí˜¸ëŠ” '{mask}' íŒ¨í„´ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•œë‹¤(ì˜ˆ: {', '.join(e + '.7' for e in examples)}). ")
    # ìš”ëª©í‘œ 3ìë¦¬ íŒíŠ¸ í¬í•¨
    base += " ë‹¤ìŒì˜ KDC 3ìë¦¬(ë¥˜Â·ê°•Â·ëª©) ëª©ë¡ì„ ì°¸ì¡°í•˜ì—¬ ê°€ì¥ ì í•©í•œ 3ìë¦¬ë¡œ ì‹œì‘í•˜ë„ë¡ í•˜ë¼: " + outline_hint
    return base

def enforce_anchor_digits(code: Optional[str], anc: Dict[str, Optional[str]]) -> Optional[str]:
    if not code:
        return code
    m = re.match(r"^(\d{1,3})(.*)$", code)
    if not m:
        return code
    head, tail = m.group(1), m.group(2)
    head = (head + "000")[:3]
    h, t, u = list(head)
    if anc.get("hundreds"): h = anc["hundreds"]
    if anc.get("tens"):     t = anc["tens"]
    if anc.get("units"):    u = anc["units"]
    return f"{h}{t}{u}" + tail

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš”ëª©í‘œ ê¸°ë°˜ ê·œì¹™ ì ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def score_outline_candidates(info: BookInfo, anchors: Dict[str, Optional[str]]) -> List[Dict[str, Any]]:
    """
    ì±… í…ìŠ¤íŠ¸ì—ì„œ ìš”ëª©í‘œ í‚¤ì›Œë“œ ë§¤ì¹­ â†’ 3ìë¦¬ ì½”ë“œë³„ ì ìˆ˜ ì‚°ì¶œ
    ë¦¬í„´: [{"code":"813","label":"ì†Œì„¤", "hits":["ì†Œì„¤"...], "score":0.83}, ...]
    """
    text = f"{info.title} {info.category} {info.description[:800]}".lower()
    pool = outline_slice_by_ryu(anchors.get("hundreds"))
    # ì‹­/ì¼ ìë¦¬ê°€ ê³ ì •ëœ ê²½ìš° í•´ë‹¹ ë²”ìœ„ë¡œ ë” ì¢í˜
    tfix, ufix = anchors.get("tens"), anchors.get("units")
    if tfix:
        pool = {k:v for k,v in pool.items() if len(k)==3 and k[1]==tfix}
    if ufix:
        pool = {k:v for k,v in pool.items() if len(k)==3 and k[2]==ufix}
    scored = []
    for code3, spec in pool.items():
        terms = spec.get("terms", [])
        hits = sorted({w for w in terms if w and w.lower() in text})
        if not hits:
            continue
        # ê°„ë‹¨ ê°€ì¤‘ì¹˜: titleì— ìˆìœ¼ë©´ 2ì , category 1.5, desc 1.0 (ëŒ€ëµ)
        t = info.title.lower()
        c = info.category.lower()
        d = (info.description or "").lower()
        s = 0.0
        for h in hits:
            s += (2.0 if h in t else 0.0) + (1.5 if h in c else 0.0) + (1.0 if h in d else 0.0)
        scored.append({"code": code3, "label": spec.get("label",""), "hits": hits, "score": s})
    # ì ìˆ˜ ì •ê·œí™”
    if scored:
        mx = max(x["score"] for x in scored) or 1.0
        for x in scored:
            x["conf"] = round(x["score"]/mx, 4)
    scored.sort(key=lambda x: (x.get("conf",0), x.get("score",0)), reverse=True)
    return scored[:12]

def make_outline_hint(cands: List[Dict[str,Any]]) -> str:
    """
    LLM í”„ë¡¬í”„íŠ¸ì— ë„£ì„ 3ìë¦¬ íŒíŠ¸ ë¬¸ìì—´. ë„ˆë¬´ ê¸¸ë©´ ìƒìœ„ 12ê°œë§Œ.
    í˜•ì‹: '813 ì†Œì„¤; 814 ìˆ˜í•„; 821 í•œêµ­ë¬¸í•™; ...'
    """
    if not cands:
        return "; ".join([f"{k} {v['label']}" for k,v in list(KDC3.items())[:15]])
    return "; ".join([f"{c['code']} {c['label']}" for c in cands])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•Œë¼ë”˜ API/ì›¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def aladin_lookup_by_api(isbn13: str, ttbkey: str) -> Optional[BookInfo]:
    if not ttbkey:
        return None
    params = {
        "ttbkey": ttbkey, "itemIdType": "ISBN13", "ItemId": isbn13,
        "output": "js", "Version": "20131101",
        "OptResult": "authors,categoryName,fulldescription,toc,packaging,ratings"
    }
    try:
        r = requests.get(ALADIN_LOOKUP_URL, params=params, headers=HEADERS, timeout=15)
        r.raise_for_status()
        data = r.json(); items = data.get("item", [])
        if not items:
            st.info("ì•Œë¼ë”˜ API(ItemLookUp)ì—ì„œ ê²°ê³¼ ì—†ìŒ â†’ ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„"); return None
        it = items[0]
        return BookInfo(
            title=clean_text(it.get("title")), author=clean_text(it.get("author")),
            pub_date=clean_text(it.get("pubDate")), publisher=clean_text(it.get("publisher")),
            isbn13=clean_text(it.get("isbn13")) or isbn13, category=clean_text(it.get("categoryName")),
            description=clean_text(it.get("fulldescription")) or clean_text(it.get("description")),
            toc=clean_text(it.get("toc")), extra=it,
        )
    except Exception as e:
        st.info(f"ì•Œë¼ë”˜ API í˜¸ì¶œ ì˜ˆì™¸ â†’ {e} / ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„"); return None

def aladin_lookup_by_web(isbn13: str) -> Optional[BookInfo]:
    try:
        params = {"SearchTarget": "Book", "SearchWord": f"isbn:{isbn13}"}
        sr = requests.get(ALADIN_SEARCH_URL, params=params, headers=HEADERS, timeout=15)
        sr.raise_for_status()
        soup = BeautifulSoup(sr.text, "html.parser")
        link_tag = soup.select_one("a.bo3")
        item_url = None
        if link_tag and link_tag.get("href"):
            item_url = urllib.parse.urljoin("https://www.aladin.co.kr", link_tag["href"])
        if not item_url:
            m = re.search(r'href=[\'\"](/shop/wproduct\.aspx\?ItemId=\d+[^\'\"]*)[\'\"]', sr.text, re.I)
            if m: item_url = urllib.parse.urljoin("https://www.aladin.co.kr", html.unescape(m.group(1)))
        if not item_url:
            first_card = soup.select_one(".ss_book_box, .ss_book_list")
            if first_card:
                a = first_card.find("a", href=True)
                if a: item_url = urllib.parse.urljoin("https://www.aladin.co.kr", a["href"])
        if not item_url:
            st.warning("ì•Œë¼ë”˜ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            with st.expander("ë””ë²„ê·¸: ê²€ìƒ‰ í˜ì´ì§€ HTML ì¼ë¶€"):
                st.code(sr.text[:2000])
            return None
        pr = requests.get(item_url, headers=HEADERS, timeout=15)
        pr.raise_for_status()
        psoup = BeautifulSoup(pr.text, "html.parser")
        og_title = psoup.select_one('meta[property="og:title"]')
        og_desc  = psoup.select_one('meta[property="og:description"]')
        title = clean_text(og_title["content"]) if og_title and og_title.has_attr("content") else ""
        desc  = clean_text(og_desc["content"]) if og_desc and og_desc.has_attr("content") else ""
        body_text = clean_text(psoup.get_text(" "))[:4000]
        description = desc or body_text
        author = publisher = pub_date = cat_text = ""
        info_box = psoup.select_one("#Ere_prod_allwrap, #Ere_prod_mconts_wrap, #Ere_prod_titlewrap")
        if info_box:
            text = clean_text(info_box.get_text(" "))
            m_author = re.search(r"(ì €ì|ì§€ì€ì´)\s*:\s*([^\|Â·/]+)", text)
            m_publisher = re.search(r"(ì¶œíŒì‚¬)\s*:\s*([^\|Â·/]+)", text)
            m_pubdate = re.search(r"(ì¶œê°„ì¼|ì¶œíŒì¼)\s*:\s*([0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2})", text)
            if m_author:   author   = clean_text(m_author.group(2))
            if m_publisher: publisher = clean_text(m_publisher.group(2))
            if m_pubdate:  pub_date = clean_text(m_pubdate.group(2))
        crumbs = psoup.select(".location, .path, .breadcrumb")
        if crumbs: cat_text = clean_text(" > ".join(c.get_text(" ") for c in crumbs))
        with st.expander("ë””ë²„ê·¸: ìŠ¤í¬ë ˆì´í•‘ ì§„ì… URL / íŒŒì‹± ê²°ê³¼"):
            st.write({"item_url": item_url, "title": title})
        return BookInfo(title=title, description=description, isbn13=isbn13,
                        author=author, publisher=publisher, pub_date=pub_date, category=cat_text)
    except Exception as e:
        st.error(f"ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì˜ˆì™¸: {e}"); return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc(book: BookInfo, api_key: str, model: str, anchor_clause: str) -> Optional[str]:
    if not api_key:
        raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°” ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ë„ì„œ ì •ë³´ë¥¼ ë³´ê³  KDC ë¶„ë¥˜ê¸°í˜¸ë¥¼ 'ìˆ«ìë§Œ' ì¶œë ¥í•´ë¼. "
        "í˜•ì‹ ì˜ˆì‹œ: 813.7 / 325.1 / 005 / 181 ë“±. ì„¤ëª…, ì ‘ë‘/ì ‘ë¯¸ í…ìŠ¤íŠ¸, ê¸°íƒ€ ë¬¸ìëŠ” ê¸ˆì§€."
        + anchor_clause
    )
    payload = {
        "title": book.title, "author": book.author, "publisher": book.publisher, "pub_date": book.pub_date,
        "isbn13": book.isbn13, "category": book.category,
        "description": book.description[:1200], "toc": book.toc[:800]
    }
    user_prompt = "ë„ì„œ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) + "\n\nKDC ìˆ«ìë§Œ ì¶œë ¥:"
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={"model": model,"messages":[{"role":"system","content":sys_prompt},
                                             {"role":"user","content":user_prompt}],
                  "temperature":0.0,"max_tokens":16},
            timeout=30,
        )
        resp.raise_for_status()
        text = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        return first_match_number(text)
    except Exception as e:
        st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}"); return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ JSON íŒŒì‹± ë³´ê°• ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_json_object(text: str) -> Optional[str]:
    if not text: return None
    m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", text, re.I)
    if m: return m.group(1)
    start = text.find("{")
    if start == -1: return None
    depth = 0; in_str = False; esc = False
    for i in range(start, len(text)):
        ch = text[i]
        if in_str:
            if esc: esc = False
            elif ch == "\\": esc = True
            elif ch == '"': in_str = False
        else:
            if ch == '"': in_str = True
            elif ch == "{": depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0: return text[start:i+1]
    return None

def _sanitize_json(s: str) -> str:
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = re.sub(r"```.*?```", "", s, flags=re.S)
    s = re.sub(r",\s*([}\]])", r"\1", s)
    s = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f]", "", s)
    return s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¼ê±°/ìˆœìœ„ JSON íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc_ranking(book: BookInfo, api_key: str, model: str, anchor_clause: str) -> Optional[List[Dict[str, Any]]]:
    if not api_key:
        return None
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ë„ì„œ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ìƒìœ„ í›„ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ë¼. "
        'ë°˜ë“œì‹œ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ì§€ì¼œë¼: {"candidates":[{"code":str,"confidence":number,'
        '"evidence_terms":[str...],"_view":str,"factors":{"title":number,"category":number,'
        '"author":number,"publisher":number,"desc":number,"toc":number}}]} '
        "ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€. ì½”ë“œíœìŠ¤ ê¸ˆì§€. ë°°ì—´ ê¸¸ì´ëŠ” 3~5. " + anchor_clause
    )
    payload = {"title": book.title,"author": book.author,"publisher": book.publisher,"pub_date": book.pub_date,
               "isbn13": book.isbn13,"category": book.category,"description": book.description[:1200],"toc": book.toc[:800]}
    user_prompt = ("ë„ì„œ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) +
                   "\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒìœ„ í›„ë³´ 3~5ê°œë¥¼ confidence ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì‚°ì¶œí•´, ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ ë°˜í™˜í•´.")
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={"model": model,"messages":[{"role":"system","content":sys_prompt},
                                             {"role":"user","content":user_prompt}],
                  "temperature":0.0,"max_tokens":520},
            timeout=30,
        )
        text = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        raw = _extract_json_object(text) or text
        safe = _sanitize_json(raw)
        parsed = json.loads(safe)
        cands = parsed.get("candidates") if isinstance(parsed, dict) else None
        if isinstance(cands, list) and cands:
            for c in cands:
                if "confidence" in c:
                    try: c["confidence"] = float(c["confidence"])
                    except Exception: pass
                fx = c.get("factors")
                if isinstance(fx, dict):
                    for k,v in list(fx.items()):
                        try: fx[k] = float(v)
                        except Exception: pass
            try: cands = sorted(cands, key=lambda x: float(x.get("confidence",0)), reverse=True)
            except Exception: pass
            return cands
        st.info("ê·¼ê±°/ìˆœìœ„ JSON: candidatesê°€ ë¹„ì–´ ìˆê±°ë‚˜ í˜•ì‹ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    except json.JSONDecodeError as je:
        st.warning(f"ê·¼ê±°/ìˆœìœ„ JSON ìƒì„± ì‹¤íŒ¨(JSONDecode): {je}"); return None
    except Exception as e:
        st.info(f"ê·¼ê±°/ìˆœìœ„ JSON ìƒì„± ì‹¤íŒ¨: {e}"); return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_kdc_from_isbn(isbn13: str, ttbkey: Optional[str], openai_key: str, model: str) -> Dict[str, Any]:
    # 0) EA â†’ last3 & ìë¦¬ë³„ ì•µì»¤
    last3 = get_ea_add_code_last3(isbn13, NLK_API_KEY)
    anchors = build_anchor_from_last3(last3)

    # 1) ì•Œë¼ë”˜
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)
    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return {"code": None, "anchors": anchors, "ea_add_last3": last3, "ranking": None,
                "signals": None, "llm_raw": None, "outline_rank": None}

    # 2) ìš”ëª©í‘œ 3ìë¦¬ ê·œì¹™ ê¸°ë°˜ í›„ë³´
    outline_rank = score_outline_candidates(info, anchors)
    outline_hint = make_outline_hint(outline_rank)

    # 3) LLM (ì•µì»¤ + ìš”ëª©í‘œ íŒíŠ¸ ë°˜ì˜)
    anchor_clause = anchor_clause_for_prompt(anchors, outline_hint)
    llm_raw = ask_llm_for_kdc(info, api_key=openai_key, model=model, anchor_clause=anchor_clause)
    ranking  = ask_llm_for_kdc_ranking(info, api_key=openai_key, model=model, anchor_clause=anchor_clause)

    # 4) ìë¦¬ì•µì»¤ ê°•ì œ ë³´ì •
    code = enforce_anchor_digits(llm_raw, anchors)

    # 5) ë””ë²„ê·¸ ì…ë ¥
    with st.expander("LLM ì…ë ¥ ì •ë³´(í™•ì¸ìš©)"):
        st.json({
            "title": info.title, "author": info.author, "publisher": info.publisher, "pub_date": info.pub_date,
            "isbn13": info.isbn13, "category": info.category,
            "description": (info.description[:600] + "â€¦") if info.description and len(info.description) > 600 else info.description,
            "toc": info.toc, "ea_add_last3": last3, "anchors": anchors,
            "outline_hint(ìƒìœ„)": outline_hint, "llm_raw": llm_raw,
        })

    signals = {"title": info.title[:120], "category": info.category[:120], "author": info.author[:80], "publisher": info.publisher[:80]}
    return {"code": code, "anchors": anchors, "ea_add_last3": last3, "ranking": ranking,
            "signals": signals, "llm_raw": llm_raw, "outline_rank": outline_rank}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“š ISBN â†’ KDC ì¶”ì²œ (EA ìë¦¬ì•µì»¤ + ìš”ëª©í‘œ 3ìë¦¬ + ì•Œë¼ë”˜ + ì±—G)")
st.caption("â‘  EA_ADD_CODE ë’¤ 3ìë¦¬ì—ì„œ 0ì´ ì•„ë‹Œ ìë¦¬ ê³ ì • â†’ â‘¡ ìš”ëª©í‘œ(3ìë¦¬) ë°˜ì˜ â†’ â‘¢ ì•Œë¼ë”˜ ìˆ˜ì§‘ â†’ â‘£ ì±—G ë„ì¶œ")

isbn = st.text_input("ISBN-13 ì…ë ¥", placeholder="ì˜ˆ: 9791193904565").strip()
go = st.button("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œ")

if go:
    if not isbn:
        st.warning("ISBNì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        norm = normalize_isbn13(isbn)
        if not norm or len(norm) != 13:
            st.info("ISBN-13 í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        with st.spinner("EA ìë¦¬ì•µì»¤ í™•ì¸ â†’ ìš”ëª©í‘œ ë¡œë”© â†’ ì•Œë¼ë”˜ ì •ë³´ ìˆ˜ì§‘ â†’ ì±—G íŒë‹¨â€¦"):
            result = get_kdc_from_isbn(isbn13=norm or isbn, ttbkey=ALADIN_TTBKEY, openai_key=OPENAI_API_KEY, model=MODEL)

        st.subheader("ê²°ê³¼")
        last3 = result.get("ea_add_last3")
        anchors = result.get("anchors") or {}
        pattern = anchors.get("pattern", "x-x-x")
        if last3:
            st.markdown(f"- **EA_ADD_CODE ë’¤ 3ìë¦¬**: `{last3}` â†’ **ìë¦¬ì•µì»¤ íŒ¨í„´**: `{pattern}`")
        else:
            st.markdown("- **EA_ADD_CODE**: ì¡°íšŒ ì‹¤íŒ¨(ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰)")
        code = result.get("code")
        if code:
            st.markdown(f"### âœ… ì¶”ì²œ KDC: **`{code}`**")
            st.caption("â€» ìë¦¬ì•µì»¤(ë°±/ì‹­/ì¼ì˜ ìë¦¬) ì œì•½ê³¼ â€˜ìš”ëª©í‘œ 3ìë¦¬â€™ë¥¼ ë°˜ì˜í•´ LLM ê²°ê³¼ë¥¼ ë³´ì •í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.error("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ISBN/í‚¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¼ê±°/ìˆœìœ„Â·ì¡°í•© + ì„¸ë¶€ ìš”ì†Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        st.markdown("#### ğŸ” ì¶”ì²œ ê·¼ê±° (ìš”ëª©í‘œ 3ìë¦¬ + LLM í›„ë³´ + ìš”ì†Œ ê°€ì¤‘ì¹˜)")
        sig = result.get("signals") or {}
        ranking = result.get("ranking") or []
        llm_raw = result.get("llm_raw")
        outline_rank = result.get("outline_rank") or []

        st.markdown(f"- **EA ìë¦¬ì•µì»¤**: ë°±={anchors.get('hundreds') or 'x'}, ì‹­={anchors.get('tens') or 'x'}, ì¼={anchors.get('units') or 'x'} (íŒ¨í„´ `{pattern}`)")
        st.markdown(f"- **LLM ì›ì¶œë ¥**: `{llm_raw or '-'}` â†’ ì•µì»¤ ë³´ì • â†’ `{code or '-'}`")
        st.markdown(f"- **ì‚¬ìš© ë©”íƒ€ë°ì´í„°**: ì œëª©='{sig.get('title','')}', ì¹´í…Œê³ ë¦¬='{sig.get('category','')}', ì €ì='{sig.get('author','')}', ì¶œíŒì‚¬='{sig.get('publisher','')}'")

        # 1) ìš”ëª©í‘œ 3ìë¦¬ ê·œì¹™ í›„ë³´ í‘œ
        import pandas as _pd
        if outline_rank:
            rows_rb = []
            for i, c in enumerate(outline_rank, start=1):
                rows_rb.append({
                    "ìˆœìœ„(RB)": i,
                    "KDC(3ìë¦¬)": c.get("code"),
                    "ë¼ë²¨": c.get("label",""),
                    "í‚¤ì›Œë“œ ì ì¤‘": ", ".join(c.get("hits",[])[:10]),
                    "ê·œì¹™ ì‹ ë¢°ë„": f"{c.get('conf',0)*100:.1f}%"
                })
            df_rb = _pd.DataFrame(rows_rb)
            st.markdown("**ìš”ëª©í‘œ(3ìë¦¬) ê¸°ë°˜ ê·œì¹™ í›„ë³´**")
            st.dataframe(df_rb, use_container_width=True)
        else:
            st.caption("ìš”ëª©í‘œ ê¸°ë°˜ ê·œì¹™ í›„ë³´: ì ì¤‘ ì—†ìŒ")

        # 2) LLM í›„ë³´ í‘œ
        if ranking:
            rows = []
            for i, c in enumerate(ranking, start=1):
                code_i = c.get("code") or ""
                conf = c.get("confidence")
                try: conf_pct = f"{float(conf)*100:.1f}%"
                except Exception: conf_pct = ""
                factors = c.get("factors", {}) if isinstance(c.get("factors"), dict) else {}
                rows.append({
                    "ìˆœìœ„(LLM)": i,
                    "KDC í›„ë³´": code_i,
                    "ì‹ ë¢°ë„": conf_pct,
                    "ê·¼ê±° í‚¤ì›Œë“œ": ", ".join((c.get("evidence_terms") or [])[:8]),
                    "ê°€ì¤‘ì¹˜(title/category/author/publisher/desc/toc)": ", ".join(
                        [f"{k}:{factors.get(k):.2f}" for k in ["title","category","author","publisher","desc","toc"]
                         if isinstance(factors.get(k), (int, float))]
                    ) or "-",
                })
            df = _pd.DataFrame(rows)
            st.markdown("**LLM ìƒìœ„ í›„ë³´**")
            st.dataframe(df, use_container_width=True)
        else:
            st.caption("LLM í›„ë³´: ìƒì„± ì•ˆ ë¨ (JSON ì‹¤íŒ¨/ì •ë³´ ë¶€ì¡±)")
