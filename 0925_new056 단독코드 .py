# new056.py (KDC3 ë‚´ì¥ + EA ìë¦¬ì•µì»¤ + ì•Œë¼ë”˜ + ìš”ëª©í‘œ ê°•ì œ + ì œë„ˆë¦­ ë¼ë²¨ í•„í„° + ê·¼ê±°í‘œ)
# - KDC3ë¥¼ ì½”ë“œì— ë‚´ì¥(000~999 ì „ë¶€ í‚¤ ì¡´ì¬) â†’ ì¼ë¶€ ëŒ€í‘œ 3ìë¦¬ëŠ” ì •í™• ë¼ë²¨/í‚¤ì›Œë“œ, ë‚˜ë¨¸ì§€ëŠ” ìë™í™•ì¥ìœ¼ë¡œ "ë¥˜ ì¼ë°˜/ì„¸ë¶€" ì²˜ë¦¬
# - EA_ADD_CODE ë’¤ 3ìë¦¬ì—ì„œ 0ì´ ì•„ë‹Œ ê° ìë¦¬(ë°±/ì‹­/ì¼)ë¥¼ ì•µì»¤ë¡œ ê³ ì •
# - ì•Œë¼ë”˜ì—ì„œ ì„œì§€ í™•ë³´ â†’ ì±—GëŠ” 'í—ˆìš© 3ìë¦¬(ìš”ëª©í‘œ)' ëª©ë¡ ì•ˆì—ì„œë§Œ ì„ íƒí•˜ë„ë¡ ê°•ì œ
# - "ì„¸ë¶€/ì¼ë°˜" ê°™ì€ ì œë„ˆë¦­ ë¼ë²¨ì€ LLM í—ˆìš©ëª©ë¡Â·ë¯¸ë¦¬ë³´ê¸°Â·ê·œì¹™ ì ìˆ˜ì—ì„œ ìµœëŒ€í•œ ë°°ì œ(í•„ìš” ì‹œ ìµœì†Œ ë³´ì¶©)
# - LLM ì¶œë ¥ ì‚¬í›„ê²€ì¦ + ê·œì¹™ê¸°ë°˜(ìš”ëª©í‘œ í‚¤ì›Œë“œ ë§¤ì¹­) ê·¼ê±° í‘œ ì œê³µ
# - ê¸°ì¡´ UI ìœ ì§€

import os
import re
import json
import html
import urllib.parse
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
from bs4 import BeautifulSoup
from pathlib import Path

import requests
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ISBN â†’ KDC ì¶”ì²œ", page_icon="ğŸ“š", layout="centered")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜/ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "gpt-4o-mini"
ALADIN_LOOKUP_URL = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx"
OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"
NLK_SEARCH_API = "https://www.nl.go.kr/NL/search/openApi/search.do"
NLK_SEOJI_API  = "https://www.nl.go.kr/seoji/SearchApi.do"

with st.expander("í™˜ê²½ì„¤ì • ë””ë²„ê·¸", expanded=True):
    st.write("ğŸ“ ì•± í´ë”:", Path(__file__).resolve().parent.as_posix())
    st.write("ğŸ” secrets.toml ì¡´ì¬?:", (Path(__file__).resolve().parent / ".streamlit" / "secrets.toml").exists())
    st.write("ğŸ”‘ st.secrets í‚¤ë“¤:", list(st.secrets.keys()))
    st.write("api_keys ë‚´ìš©:", dict(st.secrets.get("api_keys", {})))
    st.write("âœ… openai_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("openai_key")))
    st.write("âœ… aladin_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("aladin_key")))
    st.write("âœ… nlk_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("nlk_key")))

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; KDCFetcher/1.0; +https://example.local)"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ secrets.toml ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_secret(*path, default=""):
    try:
        v = st.secrets
        for p in path:
            v = v[p]
        return v
    except Exception:
        return default

OPENAI_API_KEY = (_get_secret("api_keys", "openai_key") or os.environ.get("OPENAI_API_KEY", ""))
ALADIN_TTBKEY  = (_get_secret("api_keys", "aladin_key")  or os.environ.get("ALADIN_TTBKEY", ""))
NLK_API_KEY    = (_get_secret("api_keys", "nlk_key")     or os.environ.get("NLK_API_KEY", ""))
MODEL = DEFAULT_MODEL

@dataclass
class BookInfo:
    title: str = ""
    author: str = ""
    pub_date: str = ""
    publisher: str = ""
    isbn13: str = ""
    category: str = ""
    description: str = ""
    toc: str = ""
    extra: Dict[str, Any] = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def first_match_number(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"\b([0-9]{1,3}(?:\.[0-9]+)?)\b", text)
    return m.group(1) if m else None

def strip_tags(html_text: str) -> str:
    return re.sub(r"<[^>]+>", " ", html_text)

def normalize_isbn13(isbn: str) -> str:
    s = re.sub(r"[^0-9Xx]", "", isbn or "")
    return s[-13:] if len(s) >= 13 else s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš”ëª©í‘œ(3ìë¦¬) â€” ë‚´ì¥(ëŒ€í‘œ) + ìë™í™•ì¥ìœ¼ë¡œ 000~999 ì „ì²´ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€
_KDC_HUNDREDS = {
    "0": {"label": "ì´ë¥˜",   "terms": ["ì´ë¥˜","ì§€ì‹","í•™ë¬¸","ë¬¸í—Œì •ë³´","ì„œì§€","ë°±ê³¼ì‚¬ì „","ì—°ì†ê°„í–‰ë¬¼","í•™íšŒ","ë‹¨ì²´","ê¸°ê´€","ì‹ ë¬¸","ì €ë„ë¦¬ì¦˜","ì „ì§‘","ì´ì„œ","í–¥í† ìë£Œ"]},
    "1": {"label": "ì² í•™",   "terms": ["ì² í•™","ì‚¬ìƒ","í˜•ì´ìƒí•™","ì¸ì‹ë¡ ","ë…¼ë¦¬í•™","ì‹¬ë¦¬í•™","ìœ¤ë¦¬í•™","ë™ì–‘ì² í•™","ì„œì–‘ì² í•™"]},
    "2": {"label": "ì¢…êµ",   "terms": ["ì¢…êµ","ë¶ˆêµ","ê¸°ë…êµ","ì²œì£¼êµ","ì´ìŠ¬ëŒ","íŒë‘êµ","ë„êµ","ì¢…êµì² í•™","ê²½ì „","êµë¦¬"]},
    "3": {"label": "ì‚¬íšŒê³¼í•™","terms": ["ì‚¬íšŒê³¼í•™","ê²½ì œ","ê²½ì˜","ì •ì¹˜","í–‰ì •","ë²•í•™","êµìœ¡","í†µê³„","ì‚¬íšŒë³µì§€","êµ°ì‚¬"]},
    "4": {"label": "ìì—°ê³¼í•™","terms": ["ìì—°ê³¼í•™","ìˆ˜í•™","ë¬¼ë¦¬í•™","í™”í•™","ì²œë¬¸","ì§€êµ¬ê³¼í•™","ìƒëª…ê³¼í•™","ì‹ë¬¼í•™","ë™ë¬¼í•™"]},
    "5": {"label": "ê¸°ìˆ ê³¼í•™","terms": ["ê¸°ìˆ ê³¼í•™","ì˜í•™","ê°„í˜¸","ê³µí•™","ê±´ì¶•","ê¸°ê³„","ì „ê¸°","ì „ì","ì»´í“¨í„°","í™”í•™ê³µí•™","ë†ì—…","ì‹í’ˆ","ìƒí™œê³¼í•™"]},
    "6": {"label": "ì˜ˆìˆ ",   "terms": ["ì˜ˆìˆ ","ë¯¸ìˆ ","ì¡°ê°","ê³µì˜ˆ","ì„œì˜ˆ","íšŒí™”","ë””ìì¸","ì‚¬ì§„","ìŒì•…","ê³µì—°ì˜ˆìˆ ","ì˜í™”","ì˜¤ë½","ìŠ¤í¬ì¸ "]},
    "7": {"label": "ì–¸ì–´",   "terms": ["ì–¸ì–´","ì–¸ì–´í•™","ë¬¸ë²•","ì‚¬ì „","ì‘ë¬¸","ë²ˆì—­","êµ­ì–´","ì˜ì–´","ì¼ë³¸ì–´","ì¤‘êµ­ì–´","ë…ì¼ì–´","í”„ë‘ìŠ¤ì–´","ìŠ¤í˜ì¸ì–´"]},
    "8": {"label": "ë¬¸í•™",   "terms": ["ë¬¸í•™","ë¬¸í•™ì´ë¡ ","ì‹œ","í¬ê³¡","ì†Œì„¤","ìˆ˜í•„","ì—°ì„¤","ì¼ê¸°","ì„œê°„","ê¸°í–‰","í’ì","ìœ ë¨¸","ë¥´í¬","í•œêµ­ë¬¸í•™","ì˜ë¯¸ë¬¸í•™"]},
    "9": {"label": "ì—­ì‚¬",   "terms": ["ì—­ì‚¬","ì„¸ê³„ì‚¬","í•œêµ­ì‚¬","ì¤‘êµ­ì‚¬","ì¼ë³¸ì‚¬","ìœ ëŸ½ì‚¬","ì•„í”„ë¦¬ì¹´ì‚¬","ì•„ë©”ë¦¬ì¹´ì‚¬","ì˜¤ì„¸ì•„ë‹ˆì•„","ì§€ë¦¬","ì „ê¸°","ì§€ë„"]},
}

_KDC_EXPLICIT_3 = {
    # 000ëŒ€
    "000":{"label":"ì´ë¥˜","terms":["ì´ë¥˜","ì¼ë°˜","ì§€ì‹"]},
    "010":{"label":"ë„ì„œí•™Â·ì„œì§€í•™","terms":["ë„ì„œí•™","ì„œì§€í•™","ì„œì§€"]},
    "020":{"label":"ë¬¸í—Œì •ë³´í•™","terms":["ë¬¸í—Œì •ë³´í•™","ë„ì„œê´€í•™","ì •ë³´ì¡°ì§","ë¶„ë¥˜","ëª©ë¡","ë©”íƒ€ë°ì´í„°"]},
    "030":{"label":"ë°±ê³¼ì‚¬ì „","terms":["ë°±ê³¼ì‚¬ì „","ëŒ€ë°±ê³¼"]},
    "040":{"label":"ì¼ë°˜ ì—°ì†ê°„í–‰ë¬¼","terms":["ì—°ì†ê°„í–‰ë¬¼","ì¡ì§€","ì €ë„"]},
    "050":{"label":"ì¼ë°˜ í•™íšŒÂ·ë‹¨ì²´","terms":["í•™íšŒ","ë‹¨ì²´","í˜‘íšŒ"]},
    "060":{"label":"ì¼ë°˜ ê¸°ê´€","terms":["ê¸°ê´€","ì •ë¶€ê¸°ê´€","ì—°êµ¬ì†Œ"]},
    "070":{"label":"ì‹ ë¬¸Â·ì–¸ë¡ Â·ì €ë„ë¦¬ì¦˜","terms":["ì‹ ë¬¸","ì–¸ë¡ ","ë°©ì†¡","ë‰´ìŠ¤","ì €ë„ë¦¬ì¦˜"]},
    "080":{"label":"ì „ì§‘Â·ì´ì„œ","terms":["ì „ì§‘","ì´ì„œ","ì„ ì§‘"]},
    "090":{"label":"í–¥í† ìë£Œ","terms":["í–¥í† ìë£Œ","ì§€ì—­ìë£Œ"]},

    # 100ëŒ€
    "100":{"label":"ì² í•™","terms":["ì² í•™","ì‚¬ìƒ"]},
    "110":{"label":"í˜•ì´ìƒí•™","terms":["í˜•ì´ìƒí•™","ì¡´ì¬ë¡ "]},
    "120":{"label":"ì¸ì‹ë¡ Â·ì¸ê°„í•™","terms":["ì¸ì‹ë¡ ","ì¸ê°„í•™"]},
    "130":{"label":"ì² í•™ì˜ ì²´ê³„","terms":["ì²´ê³„"]},
    "140":{"label":"ê²½í•™","terms":["ê²½í•™"]},
    "150":{"label":"ë™ì–‘ì² í•™","terms":["ë™ì–‘ì² í•™","ìœ êµ","ë¶ˆêµì² í•™"]},
    "160":{"label":"ì„œì–‘ì² í•™","terms":["ì„œì–‘ì² í•™","ì‹¤ì¡´ì£¼ì˜","í˜„ìƒí•™"]},
    "170":{"label":"ë…¼ë¦¬í•™","terms":["ë…¼ë¦¬í•™","ì¶”ë¡ "]},
    "180":{"label":"ì‹¬ë¦¬í•™","terms":["ì‹¬ë¦¬í•™","ì¸ì§€","ê°ì •","í–‰ë™"]},
    "190":{"label":"ìœ¤ë¦¬í•™, ë„ë•ì² í•™","terms":["ìœ¤ë¦¬í•™","ë„ë•"]},

    # 200ëŒ€
    "200":{"label":"ì¢…êµ","terms":["ì¢…êµ","ì‹ ì•™"]},
    "210":{"label":"ë¹„êµì¢…êµ","terms":["ë¹„êµì¢…êµ","ì¢…êµì‚¬"]},
    "220":{"label":"ë¶ˆêµ","terms":["ë¶ˆêµ","ê²½ì „","ì„ ì¢…","ëŒ€ìŠ¹"]},
    "230":{"label":"ê¸°ë…êµ","terms":["ê¸°ë…êµ","ì„±ê²½","ì‹ í•™"]},
    "240":{"label":"ë„êµ","terms":["ë„êµ"]},
    "250":{"label":"ì²œë„êµ","terms":["ì²œë„êµ"]},
    "270":{"label":"íŒë‘êµ, ë¸Œë¼ë§Œêµ","terms":["íŒë‘êµ","ë¸Œë¼ë§Œêµ"]},
    "280":{"label":"ì´ìŠ¬ëŒêµ","terms":["ì´ìŠ¬ëŒ","ê¾¸ë€"]},
    "290":{"label":"ê¸°íƒ€ ì¢…êµ","terms":["íŒë‘êµ","ë„êµ","ì‹ í¥ì¢…êµ"]},

    # 300ëŒ€
    "300":{"label":"ì‚¬íšŒê³¼í•™","terms":["ì‚¬íšŒê³¼í•™"]},
    "310":{"label":"í†µê³„ìë£Œ","terms":["í†µê³„","ë°ì´í„°"]},
    "320":{"label":"ê²½ì œí•™","terms":["ê²½ì œí•™","ê±°ì‹œê²½ì œ","ë¯¸ì‹œê²½ì œ"]},
    "330":{"label":"ì‚¬íšŒí•™Â·ì‚¬íšŒë¬¸ì œ","terms":["ì‚¬íšŒí•™","ì‚¬íšŒë¬¸ì œ","ë³µì§€"]},
    "340":{"label":"ì •ì¹˜í•™","terms":["ì •ì¹˜","ì™¸êµ"]},
    "350":{"label":"í–‰ì •í•™","terms":["í–‰ì •","ê³µê³µê´€ë¦¬"]},
    "360":{"label":"ë²•í•™","terms":["ë²•í•™","í—Œë²•","í˜•ë²•","ë¯¼ë²•"]},
    "370":{"label":"êµìœ¡í•™","terms":["êµìœ¡í•™","êµìœ¡ê³¼ì •","í‰ê°€"]},
    "380":{"label":"í’ìŠµ, ì˜ˆì ˆ ë¯¼ì†í•™","terms":["í’ìŠµ","ì˜ˆì ˆ","ë¯¼ì†"]},    
    "390":{"label":"êµ­ë°©Â·êµ°ì‚¬í•™","terms":["êµ°ì‚¬","ì•ˆë³´"]},

    # 400ëŒ€
    "400":{"label":"ìì—°ê³¼í•™","terms":["ìì—°ê³¼í•™"]},
    "410":{"label":"ìˆ˜í•™","terms":["ìˆ˜í•™","ëŒ€ìˆ˜","ê¸°í•˜","í•´ì„","í™•ë¥ ","í†µê³„"]},
    "420":{"label":"ë¬¼ë¦¬í•™","terms":["ë¬¼ë¦¬","ì—­í•™","ì „ìê¸°","ì–‘ì","ì—´"]},
    "430":{"label":"í™”í•™","terms":["í™”í•™","ìœ ê¸°í™”í•™","ë¬´ê¸°í™”í•™"]},
    "440":{"label":"ì²œë¬¸í•™","terms":["ì²œë¬¸","ìš°ì£¼","í–‰ì„±"]},
    "450":{"label":"ì§€í•™","terms":["ì§€êµ¬ê³¼í•™","ì§€ì§ˆ","ê¸°ìƒ","í•´ì–‘"]},
    "460":{"label":"ê´‘ë¬¼í•™","terms":["ê´‘ë¬¼","ì›ì„"]},
    "470":{"label":"ìƒëª…ê³¼í•™","terms":["ìƒëª…ê³¼í•™","ìƒë¬¼","ìœ ì „","ë¶„ììƒë¬¼"]},
    "480":{"label":"ì‹ë¬¼í•™","terms":["ì‹ë¬¼","ì‹ë¬¼í•™"]},
    "490":{"label":"ë™ë¬¼í•™","terms":["ë™ë¬¼","ë™ë¬¼í•™"]},

    # 500ëŒ€
    "500":{"label":"ê¸°ìˆ ê³¼í•™","terms":["ê¸°ìˆ ê³¼í•™"]},
    "510":{"label":"ì˜í•™","terms":["ì˜í•™","ë‚´ê³¼","ì™¸ê³¼","ì•½ë¦¬","ê³µì¤‘ë³´ê±´"]},
    "520":{"label":"ë†ì—…","terms":["ë†ì—…","ì„ì—…","ì›ì˜ˆ","ìˆ˜ì˜"]},
    "530":{"label":"ê³µí•™","terms":["ê³µí•™","ê³µí•™ì¼ë°˜"]},
    "550":{"label":"ê¸°ê³„ê³µí•™","terms":["ê¸°ê³„","ì œì¡°","ë©”ì¹´íŠ¸ë¡œë‹‰ìŠ¤"]},
    "560":{"label":"ì „ê¸°ê³µí•™","terms":["ì „ê¸°","ì „ì","ì „ë ¥","ëª¨í„°"]},
    "570":{"label":"í™”í•™ê³µí•™","terms":["í™”í•™ê³µí•™","ê³µì •","ì¬ë£Œ"]},
    "580":{"label":"ì œì¡°ì—…","terms":["ì œì¡°","ìƒì‚°","í’ˆì§ˆ"]},
    "590":{"label":"ìƒí™œê³¼í•™","terms":["ê°€ì •","ìƒí™œê³¼í•™","ì‹í’ˆ","ì˜ì–‘"]},

    # 600ëŒ€
    "600":{"label":"ì˜ˆìˆ ","terms":["ì˜ˆìˆ "]},
    "620":{"label":"ì¡°ê°","terms":["ì¡°ê°","ë„ìê¸°","ì¡°í˜•"]},
    "630":{"label":"ê³µì˜ˆ","terms":["ê³µì˜ˆ"]},
    "640":{"label":"ì„œì˜ˆ","terms":["ì„œì˜ˆ","ìº˜ë¦¬ê·¸ë˜í”¼"]},
    "650":{"label":"íšŒí™”","terms":["íšŒí™”","ë“œë¡œì‰","ìˆ˜ì±„","ìœ í™”","ë””ìì¸"]},
    "660":{"label":"ì‚¬ì§„ì˜ˆìˆ ","terms":["ì‚¬ì§„","ì˜ìƒ","ì´¬ì˜","í›„ë³´ì •"]},
    "670":{"label":"ìŒì•…","terms":["ìŒì•…","ì•…ê¸°","ì‘ê³¡","ì´ë¡ "]},
    "680":{"label":"ê³µì—°ì˜ˆìˆ ","terms":["ê³µì—°","ë¬´ìš©","ì—°ê·¹","ë®¤ì§€ì»¬"]},
    "690":{"label":"ì˜¤ë½Â·ìŠ¤í¬ì¸ ","terms":["ì˜¤ë½","ìŠ¤í¬ì¸ ","ê²Œì„"]},

    # 700ëŒ€
    "700":{"label":"ì–¸ì–´","terms":["ì–¸ì–´","ì–¸ì–´í•™"]},
    "710":{"label":"í•œêµ­ì–´","terms":["êµ­ì–´","í•œêµ­ì–´","ë¬¸ë²•","ë§ì¶¤ë²•","ë§í•˜ê¸°","ì“°ê¸°"]},
    "720":{"label":"ì¤‘êµ­ì–´","terms":["ì¤‘êµ­ì–´","Chinese","HSK"]},
    "730":{"label":"ì¼ë³¸ì–´","terms":["ì¼ë³¸ì–´","Japanese","JLPT"]},
    "740":{"label":"ì˜ì–´","terms":["ì˜ì–´","English","ë¬¸ë²•","íšŒí™”","ë…í•´","ì‘ë¬¸","í† ìµ","í† í”Œ"]},
    "750":{"label":"ë…ì¼ì–´","terms":["ë…ì¼ì–´","German"]},
    "760":{"label":"í”„ë‘ìŠ¤ì–´","terms":["í”„ë‘ìŠ¤ì–´","French"]},
    "770":{"label":"ìŠ¤í˜ì¸ì–´","terms":["ìŠ¤í˜ì¸ì–´","Spanish"]},
    "780":{"label":"ì´íƒˆë¦¬ì•„ì–´","terms":["ì´íƒˆë¦¬ì•„ì–´","Italia"]},
    "790":{"label":"ê¸°íƒ€ ì œì–´","terms":["ëŸ¬ì‹œì•„ì–´","ì•„ëì–´"]},

    # 800ëŒ€
    "800":{"label":"ë¬¸í•™","terms":["ë¬¸í•™"]},
    "810":{"label":"í•œêµ­ë¬¸í•™","terms":["í•œêµ­","í•œêµ­ë¬¸í•™"]},
    "820":{"label":"ì¤‘êµ­ë¬¸í•™","terms":["ì¤‘êµ­","ì¤‘êµ­ë¬¸í•™"]},
    "830":{"label":"ì¼ë³¸ë¬¸í•™","terms":["ì¼ë³¸","ì¼ë³¸ë¬¸í•™"]},   
    "840":{"label":"ì˜ë¯¸ë¬¸í•™","terms":["ì˜ë¯¸","ì˜ë¯¸ë¬¸í•™"]},
    "850":{"label":"ë…ì¼ë¬¸í•™","terms":["ë…ë¬¸í•™"]},
    "860":{"label":"í”„ë‘ìŠ¤ë¬¸í•™","terms":["ë¶ˆë¬¸í•™"]},
    "870":{"label":"ìŠ¤í˜ì¸Â·í¬ë¥´íˆ¬ê°ˆë¬¸í•™","terms":["ì„œë°˜ì•„ë¬¸í•™","ìŠ¤í˜ì¸","í¬ë¥´íˆ¬ê°ˆ"]},
    "880":{"label":"ì´íƒˆë¦¬ì•„ë¬¸í•™","terms":["ì´íƒˆë¦¬ì•„ë¬¸í•™","ì´íƒˆë¦¬ì•„"]},
    "890":{"label":"ê¸°íƒ€ ì œë¬¸í•™","terms":["ëŸ¬ì‹œì•„ë¬¸í•™","ëŸ¬ì‹œì•„","ì•„ëë¬¸í•™","ì•„ë"]},

    # 900ëŒ€
    "900":{"label":"ì—­ì‚¬","terms":["ì—­ì‚¬"]},
    "910":{"label":"ì•„ì‹œì•„","terms":["ì•„ì‹œì•„ì‚¬","ì•„ì‹œì•„"]},
    "920":{"label":"ìœ ëŸ½ì‚¬","terms":["ìœ ëŸ½ì‚¬","ìœ ëŸ½"]},
    "930":{"label":"ì•„í”„ë¦¬ì¹´","terms":["ì•„í”„ë¦¬ì¹´ì‚¬","ì•„í”„ë¦¬ì¹´"]},
    "940":{"label":"ë¶ì•„ë©”ë¦¬ì¹´","terms":["ë¶ì•„ë©”ë¦¬ì¹´ì‚¬","ë¯¸êµ­ì‚¬","ìºë‚˜ë‹¤ì‚¬","ë¶ì•„ë©”ë¦¬ì¹´"]},
    "950":{"label":"ë‚¨ì•„ë©”ë¦¬ì¹´ì‚¬","terms":["ë‚¨ì•„ë©”ë¦¬ì¹´ì‚¬"]},
    "960":{"label":"ì˜¤ì„¸ì•„ë‹ˆì•„ì‚¬","terms":["ì˜¤ì„¸ì•„ë‹ˆì•„ì‚¬","ë¶ê·¹","ë‚¨ê·¹"]},
    "980":{"label":"ì§€ë¦¬","terms":["ì§€ë¦¬","ì—¬í–‰","ì§€ë„"]},
    "990":{"label":"ì „ê¸°","terms":["ì „ê¸°","ì „ê¸°ë¬¸"]},
}

def _auto_expand_kdc3(explicit_map: dict, hundreds_map: dict) -> dict:
    """ëª…ì‹œëœ 3ìë¦¬ ì™¸ì˜ ì „ ì˜ì—­(000~999)ì„ 'ë¥˜ ì¼ë°˜/ì„¸ë¶€'ë¡œ ìë™ ë³´ì¶©í•´ ì „ì²´ ì‚¬ì „ì„ ì™„ì„±."""
    full = dict(explicit_map)
    for h in "0123456789":
        base = hundreds_map[h]
        for t in "0123456789":
            for u in "0123456789":
                code = f"{h}{t}{u}"
                if code in full:
                    continue
                label = f"{base['label']} ì¼ë°˜"
                terms = list(base["terms"])
                if t != "0" or u != "0":
                    label = f"{base['label']} ì„¸ë¶€"
                full[code] = {"label": label, "terms": terms}
    return full

# ì‹¤í–‰ ì‹œ ì „ì²´(000~999) 3ìë¦¬ ì‚¬ì „ ì™„ì„±
KDC3: Dict[str, Dict[str, Any]] = _auto_expand_kdc3(_KDC_EXPLICIT_3, _KDC_HUNDREDS)

# === NEW: ì œë„ˆë¦­ ë¼ë²¨ ì—¬ë¶€ íŒë‹¨ ===
def _is_generic_label(label: str) -> bool:
    if not label:
        return True
    lbl = str(label)
    return ("ì„¸ë¶€" in lbl) or ("ì¼ë°˜" in lbl)

# === NEW: LLMìš© í—ˆìš©ëª©ë¡(ì˜ë¯¸ ìˆëŠ” ë¼ë²¨ ìœ„ì£¼) êµ¬ì„± ===
def build_allowed_for_llm(allowed_all: Dict[str, Dict[str,Any]],
                          anchors: Dict[str, Optional[str]],
                          min_keep: int = 12) -> Dict[str, Dict[str,Any]]:
    """
    1) ì˜ë¯¸ ìˆëŠ” ë¼ë²¨(ì„¸ë¶€/ì¼ë°˜ ì•„ë‹Œ ê²ƒ)ë§Œ ìš°ì„  ì±„íƒ
    2) ë„ˆë¬´ ì ìœ¼ë©´(ì•µì»¤ë¡œ ê³¼ë„íˆ ì¢í˜€ì§„ ê²½ìš°) ì œë„ˆë¦­ ì¼ë¶€ë¥¼ ë³´ì¶©
    """
    meaningful = {k:v for k,v in allowed_all.items() if not _is_generic_label(v.get("label",""))}
    if len(meaningful) >= min_keep:
        return meaningful

    generic = {k:v for k,v in allowed_all.items() if k not in meaningful}
    # ìš°ì„ ìˆœìœ„: (ì‹­/ì¼ ìë¦¬ê°€ 0ì´ ì•„ë‹Œ ì½”ë“œ) > ë‚˜ë¨¸ì§€
    def _generic_rank(code: str) -> tuple:
        return ((code[1] != "0") + (code[2] != "0"), code)  # ì„¸ë¶„ë„, ì½”ë“œì •ë ¬
    generic_sorted = dict(sorted(generic.items(), key=lambda kv: _generic_rank(kv[0]), reverse=True))

    out = dict(meaningful)
    for k, v in generic_sorted.items():
        out[k] = v
        if len(out) >= min_keep:
            break
    return out if out else allowed_all

def outline_slice_by_anchors(anc: Dict[str, Optional[str]]) -> Dict[str, Dict[str, Any]]:
    """ìë¦¬ì•µì»¤(ë°±/ì‹­/ì¼) ì œì•½ìœ¼ë¡œ KDC3 í—ˆìš© ì§‘í•© í•„í„°."""
    pool = KDC3
    h, t, u = anc.get("hundreds"), anc.get("tens"), anc.get("units")
    if h:
        pool = {k:v for k,v in pool.items() if len(k)==3 and k[0]==h}
    if t:
        pool = {k:v for k,v in pool.items() if len(k)==3 and k[1]==t}
    if u:
        pool = {k:v for k,v in pool.items() if len(k)==3 and k[2]==u}
    return pool

def allowed_outline_hint(allowed: Dict[str, Dict[str,Any]], limit=40) -> str:
    """LLM í”„ë¡¬í”„íŠ¸ìš© í—ˆìš©ëª©ë¡ íŒíŠ¸: '813=í•œêµ­ì†Œì„¤; 814=í•œêµ­ìˆ˜í•„; ...' (ì½”ë“œ ì •ë ¬ê¸°ì¤€ ìƒìœ„ Nê°œ)."""
    items = sorted(allowed.items(), key=lambda kv: kv[0])[:limit]
    return "; ".join([f"{code}={spec.get('label','')}" for code, spec in items])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EA_ADD_CODE ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_ea_add_code_last3(isbn13: str, key: str) -> Optional[str]:
    if not key:
        st.info("NLK_API_KEYê°€ ì—†ì–´ EA_ADD_CODE ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    # 1) ì„œì§€(ISBN) API (ê¶Œì¥)
    try:
        p1 = {"cert_key": key, "result_style": "json", "page_no": 1, "page_size": 5, "isbn": isbn13}
        r1 = requests.get(NLK_SEOJI_API, params=p1, headers=HEADERS, timeout=10)
        r1.raise_for_status()
        d1 = r1.json()
        docs = d1.get("docs") if isinstance(d1, dict) else None
        if isinstance(docs, list) and docs:
            ea = docs[0].get("EA_ADD_CODE") or docs[0].get("ea_add_code")
            if ea:
                m = re.search(r"(\d{3})$", str(ea))
                if m:
                    last3 = m.group(1)
                    st.success(f"(ì„œì§€API) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                    return last3
    except Exception as e:
        st.info(f"ì„œì§€API ì‹¤íŒ¨ â†’ ì¼ë°˜ê²€ìƒ‰ ë°±ì—…: {e}")
    # 2) ì¼ë°˜ê²€ìƒ‰ ë°±ì—…
    try:
        p2 = {"key": key, "srchTarget": "total", "kwd": isbn13, "pageNum": 1, "pageSize": 1, "apiType": "json"}
        r2 = requests.get(NLK_SEARCH_API, params=p2, headers=HEADERS, timeout=10)
        r2.raise_for_status()
        d2 = r2.json()
        result = d2.get("result") if isinstance(d2, dict) else None
        if isinstance(result, list):
            result = result[0] if result else {}
        recs = None
        if isinstance(result, dict):
            recs = result.get("recordList") or result.get("recordlist") or result.get("records") or result.get("record")
        if isinstance(recs, dict):
            recs = [recs]
        if isinstance(recs, list) and recs:
            rec0 = recs[0]
            if isinstance(rec0, list) and rec0:
                rec0 = rec0[0]
            if isinstance(rec0, dict):
                ea = rec0.get("EA_ADD_CODE") or rec0.get("ea_add_code")
                if ea:
                    m = re.search(r"(\d{3})$", str(ea))
                    if m:
                        last3 = m.group(1)
                        st.success(f"(ì¼ë°˜ê²€ìƒ‰) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                        return last3
        st.warning("NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: ì‘ë‹µ êµ¬ì¡° ë¯¸ì¼ì¹˜")
        return None
    except Exception as e:
        st.warning(f"NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìë¦¬ë³„ ì•µì»¤ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_anchor_from_last3(last3: Optional[str]) -> Dict[str, Optional[str]]:
    """
    last3 ì˜ˆ: '813' â†’ ë°±=8, ì‹­=1, ì¼=3 (0ë³´ë‹¤ í° ìë¦¬ë§Œ ê³ ì •) / '800' â†’ ë°±=8ë§Œ ê³ ì •
    """
    anchors = {"hundreds": None, "tens": None, "units": None, "pattern": "x-x-x"}
    if not (last3 and len(last3) == 3 and last3.isdigit()):
        return anchors
    h, t, u = last3[0], last3[1], last3[2]
    anchors["hundreds"] = h if int(h) > 0 else None
    anchors["tens"]     = t if int(t) > 0 else None
    anchors["units"]    = u if int(u) > 0 else None
    anchors["pattern"]  = f"{anchors['hundreds'] or 'x'}-{anchors['tens'] or 'x'}-{anchors['units'] or 'x'}"
    return anchors

def enforce_anchor_digits(code: Optional[str], anc: Dict[str, Optional[str]]) -> Optional[str]:
    if not code:
        return code
    m = re.match(r"^(\d{1,3})(.*)$", code)
    if not m:
        return code
    head, tail = m.group(1), m.group(2)
    head = (head + "000")[:3]
    h, t, u = list(head)
    if anc.get("hundreds"): h = anc["hundreds"]
    if anc.get("tens"):     t = anc["tens"]
    if anc.get("units"):    u = anc["units"]
    return f"{h}{t}{u}" + tail

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•Œë¼ë”˜ API/ì›¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def aladin_lookup_by_api(isbn13: str, ttbkey: str) -> Optional[BookInfo]:
    if not ttbkey:
        return None
    params = {
        "ttbkey": ttbkey, "itemIdType": "ISBN13", "ItemId": isbn13,
        "output": "js", "Version": "20131101",
        "OptResult": "authors,categoryName,fulldescription,toc,packaging,ratings"
    }
    try:
        r = requests.get(ALADIN_LOOKUP_URL, params=params, headers=HEADERS, timeout=15)
        r.raise_for_status()
        data = r.json()
        items = data.get("item", [])
        if not items:
            st.info("ì•Œë¼ë”˜ API(ItemLookUp)ì—ì„œ ê²°ê³¼ ì—†ìŒ â†’ ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
            return None
        it = items[0]
        return BookInfo(
            title=clean_text(it.get("title")),
            author=clean_text(it.get("author")),
            pub_date=clean_text(it.get("pubDate")),
            publisher=clean_text(it.get("publisher")),
            isbn13=clean_text(it.get("isbn13")) or isbn13,
            category=clean_text(it.get("categoryName")),
            description=clean_text(it.get("fulldescription")) or clean_text(it.get("description")),
            toc=clean_text(it.get("toc")),
            extra=it,
        )
    except Exception as e:
        st.info(f"ì•Œë¼ë”˜ API í˜¸ì¶œ ì˜ˆì™¸ â†’ {e} / ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
        return None

def aladin_lookup_by_web(isbn13: str) -> Optional[BookInfo]:
    try:
        params = {"SearchTarget": "Book", "SearchWord": f"isbn:{isbn13}"}
        sr = requests.get(ALADIN_SEARCH_URL, params=params, headers=HEADERS, timeout=15)
        sr.raise_for_status()
        soup = BeautifulSoup(sr.text, "html.parser")

        link_tag = soup.select_one("a.bo3")
        item_url = None
        if link_tag and link_tag.get("href"):
            item_url = urllib.parse.urljoin("https://www.aladin.co.kr", link_tag["href"])
        if not item_url:
            m = re.search(r'href=[\'\"](/shop/wproduct\.aspx\?ItemId=\d+[^\'\"]*)[\'\"]', sr.text, re.I)
            if m:
                item_url = urllib.parse.urljoin("https://www.aladin.co.kr", html.unescape(m.group(1)))
        if not item_url:
            first_card = soup.select_one(".ss_book_box, .ss_book_list")
            if first_card:
                a = first_card.find("a", href=True)
                if a:
                    item_url = urllib.parse.urljoin("https://www.aladin.co.kr", a["href"])

        if not item_url:
            st.warning("ì•Œë¼ë”˜ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            with st.expander("ë””ë²„ê·¸: ê²€ìƒ‰ í˜ì´ì§€ HTML ì¼ë¶€"):
                st.code(sr.text[:2000])
            return None

        pr = requests.get(item_url, headers=HEADERS, timeout=15)
        pr.raise_for_status()
        psoup = BeautifulSoup(pr.text, "html.parser")

        og_title = psoup.select_one('meta[property="og:title"]')
        og_desc  = psoup.select_one('meta[property="og:description"]')
        title = clean_text(og_title["content"]) if og_title and og_title.has_attr("content") else ""
        desc  = clean_text(og_desc["content"]) if og_desc and og_desc.has_attr("content") else ""

        body_text = clean_text(psoup.get_text(" "))[:4000]
        description = desc or body_text

        author = ""
        publisher = ""
        pub_date = ""
        cat_text = ""

        info_box = psoup.select_one("#Ere_prod_allwrap, #Ere_prod_mconts_wrap, #Ere_prod_titlewrap")
        if info_box:
            text = clean_text(info_box.get_text(" "))
            m_author = re.search(r"(ì €ì|ì§€ì€ì´)\s*:\s*([^\|Â·/]+)", text)
            m_publisher = re.search(r"(ì¶œíŒì‚¬)\s*:\s*([^\|Â·/]+)", text)
            m_pubdate = re.search(r"(ì¶œê°„ì¼|ì¶œíŒì¼)\s*:\s*([0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2})", text)
            if m_author:   author   = clean_text(m_author.group(2))
            if m_publisher: publisher = clean_text(m_publisher.group(2))
            if m_pubdate:  pub_date = clean_text(m_pubdate.group(2))

        crumbs = psoup.select(".location, .path, .breadcrumb")
        if crumbs:
            cat_text = clean_text(" > ".join(c.get_text(" ") for c in crumbs))

        with st.expander("ë””ë²„ê·¸: ìŠ¤í¬ë ˆì´í•‘ ì§„ì… URL / íŒŒì‹± ê²°ê³¼"):
            st.write({"item_url": item_url, "title": title})
        
        return BookInfo(
            title=title,
            description=description,
            isbn13=isbn13,
            author=author,
            publisher=publisher,
            pub_date=pub_date,
            category=cat_text
        )
    except Exception as e:
        st.error(f"ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì˜ˆì™¸: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš”ëª©í‘œ ê¸°ë°˜ ê·œì¹™ ì ìˆ˜(ë³´ì¡° ê·¼ê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def score_outline_candidates(info: BookInfo, allowed: Dict[str, Dict[str,Any]]) -> List[Dict[str, Any]]:
    """
    í—ˆìš©ëœ 3ìë¦¬ ì§‘í•© ì•ˆì—ì„œ í…ìŠ¤íŠ¸ ë§¤ì¹­ ì ìˆ˜í™” â†’ ê·¼ê±°í‘œ ë³´ì¡°.
    ë¦¬í„´: [{"code":"813","label":"í•œêµ­ì†Œì„¤","hits":[...],"score":..,"conf":..}, ...]
    """
    text = f"{(info.title or '').lower()} {(info.category or '').lower()} {(info.description or '')[:800].lower()}"
    scored = []
    for code3, spec in allowed.items():
        terms = spec.get("terms", [])
        hits = sorted({w for w in terms if w and w.lower() in text})
        if not hits:
            continue
        t = (info.title or "").lower()
        c = (info.category or "").lower()
        d = (info.description or "").lower()
        s = 0.0
        for h in hits:
            s += (2.0 if h.lower() in t else 0.0) + (1.5 if h.lower() in c else 0.0) + (1.0 if h.lower() in d else 0.0)
        # === ì œë„ˆë¦­ ë¼ë²¨ íŒ¨ë„í‹° ===
        if _is_generic_label(spec.get("label","")):
            s *= 0.6  # 40% íŒ¨ë„í‹°
        scored.append({"code": code3, "label": spec.get("label",""), "hits": hits, "score": s})
    if scored:
        mx = max(x["score"] for x in scored) or 1.0
        for x in scored:
            x["conf"] = round(x["score"]/mx, 4)
    scored.sort(key=lambda x: (x.get("conf",0), x.get("score",0)), reverse=True)
    return scored[:12]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM í˜¸ì¶œ (ìš”ëª©í‘œ í—ˆìš©ëª©ë¡ ê°•ì œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc_with_allowed(book: BookInfo, api_key: str, model: str,
                                 anchors: Dict[str, Optional[str]],
                                 allowed: Dict[str, Dict[str,Any]]) -> Optional[str]:
    if not api_key:
        raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    rules = []
    if anchors.get("hundreds"): rules.append(f"ë°±ì˜ ìë¦¬ëŠ” {anchors['hundreds']}")
    if anchors.get("tens"):     rules.append(f"ì‹­ì˜ ìë¦¬ëŠ” {anchors['tens']}")
    if anchors.get("units"):    rules.append(f"ì¼ì˜ ìë¦¬ëŠ” {anchors['units']}")
    anchor_txt = ""
    if rules:
        mask = anchors.get("pattern","x-x-x").replace("-","")
        anchor_txt = (" ìë¦¬ ì œì•½: " + ", ".join(rules) +
                      f" â†’ ê¸°ë³¸ 3ìë¦¬ëŠ” '{mask}' íŒ¨í„´ì„ ë”°ë¼ì•¼ í•œë‹¤. ")

    allowed_hint = allowed_outline_hint(allowed, limit=60) or "(ì—†ìŒ)"
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ì„œì§€ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ KDC ë¶„ë¥˜ê¸°í˜¸ë¥¼ 'ìˆ«ìë§Œ' ì¶œë ¥í•˜ë¼. "
        "ìµœëŒ€ í•œ ì¤„, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€. "
        + anchor_txt +
        " ë°˜ë“œì‹œ ê¸°ë³¸ 3ìë¦¬ëŠ” ì•„ë˜ 'í—ˆìš© ëª©ë¡' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤(ëª©ë¡ ë°–ì€ ë¬´íš¨). "
        f"í—ˆìš© ëª©ë¡(ì½”ë“œ=ë¼ë²¨): {allowed_hint} "
        "ì˜ˆ) 813.7 / 325.1 / 005 / 181 ê³¼ ê°™ì€ í˜•íƒœë¡œ ìˆ«ìë§Œ."
    )
    payload = {
        "title": book.title, "author": book.author, "publisher": book.publisher, "pub_date": book.pub_date,
        "isbn13": book.isbn13, "category": book.category,
        "description": (book.description or "")[:1200], "toc": (book.toc or "")[:800]
    }
    user_prompt = "ì„œì§€ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) + "\n\nKDC ìˆ«ìë§Œ:"
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={"model": model,
                  "messages": [{"role":"system","content":sys_prompt},
                               {"role":"user","content":user_prompt}],
                  "temperature":0.0, "max_tokens":16},
            timeout=30,
        )
        resp.raise_for_status()
        text = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        return first_match_number(text)
    except Exception as e:
        st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM í›„ë³´(ê·¼ê±°í‘œì‹œìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc_ranking(book: BookInfo, api_key: str, model: str,
                            anchors: Dict[str, Optional[str]],
                            allowed: Dict[str, Dict[str,Any]]) -> Optional[List[Dict[str, Any]]]:
    if not api_key:
        return None
    rules = []
    if anchors.get("hundreds"): rules.append(f"ë°±={anchors['hundreds']}")
    if anchors.get("tens"):     rules.append(f"ì‹­={anchors['tens']}")
    if anchors.get("units"):    rules.append(f"ì¼={anchors['units']}")
    allowed_hint = allowed_outline_hint(allowed, limit=60) or "(ì—†ìŒ)"
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ìƒìœ„ í›„ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ë¼. "
        'ìŠ¤í‚¤ë§ˆ: {"candidates":[{"code":str,"confidence":number,"evidence_terms":[str...],'
        '"_view":str,"factors":{"title":number,"category":number,"author":number,"publisher":number,"desc":number,"toc":number}}]} '
        "ë°˜ë“œì‹œ ê¸°ë³¸ 3ìë¦¬ëŠ” ë‹¤ìŒ í—ˆìš© ëª©ë¡ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤(ëª©ë¡ ë°– ê¸ˆì§€). "
        f"í—ˆìš© ëª©ë¡: {allowed_hint}. ìë¦¬ ì œì•½: {', '.join(rules) if rules else 'ì—†ìŒ'}."
        " ì¶”ê°€ í…ìŠ¤íŠ¸/ì½”ë“œíœìŠ¤ ê¸ˆì§€. í›„ë³´ 3~5ê°œ."
    )
    payload = {"title": book.title,"author": book.author,"publisher": book.publisher,"pub_date": book.pub_date,
               "isbn13": book.isbn13,"category": book.category,
               "description": (book.description or "")[:1200], "toc": (book.toc or "")[:800]}
    user_prompt = "ì„œì§€ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) + "\n\nJSONë§Œ ë°˜í™˜:"
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={"model": model, "messages":[{"role":"system","content":sys_prompt},
                                              {"role":"user","content":user_prompt}],
                  "temperature":0.0, "max_tokens":520},
            timeout=30,
        )
        text = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        raw = text[text.find("{"): text.rfind("}")+1] if "{" in text and "}" in text else text
        raw = raw.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
        raw = re.sub(r",\s*([}\]])", r"\1", raw)
        parsed = json.loads(raw)
        cands = parsed.get("candidates") if isinstance(parsed, dict) else None
        if isinstance(cands, list) and cands:
            for c in cands:
                if "confidence" in c:
                    try: c["confidence"] = float(c["confidence"])
                    except: pass
                if isinstance(c.get("factors"), dict):
                    for k,v in list(c["factors"].items()):
                        try: c["factors"][k] = float(v)
                        except: pass
            try: cands = sorted(cands, key=lambda x: float(x.get("confidence",0)), reverse=True)
            except: pass
            return cands
        return None
    except Exception as e:
        st.info(f"ê·¼ê±°/ìˆœìœ„ JSON ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_kdc_from_isbn(isbn13: str, ttbkey: Optional[str], openai_key: str, model: str) -> Dict[str, Any]:
    # 0) EA â†’ last3 & ìë¦¬ì•µì»¤
    last3 = get_ea_add_code_last3(isbn13, NLK_API_KEY)
    anchors = build_anchor_from_last3(last3)

    # 1) ì•Œë¼ë”˜
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)
    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return {"code": None, "anchors": anchors, "ea_add_last3": last3,
                "ranking": None, "signals": None, "llm_raw": None,
                "allowed_size": 0, "allowed_preview": "", "outline_rank": None}

    # 2) í—ˆìš© ê°€ëŠ¥í•œ 3ìë¦¬(ìš”ëª©í‘œ) ì§‘í•© êµ¬ì„± (ìë¦¬ì•µì»¤ë¡œ í•„í„°)
    allowed_all = outline_slice_by_anchors(anchors)
    allowed_for_llm = build_allowed_for_llm(allowed_all, anchors, min_keep=12)

    allowed_set_all = set(allowed_all.keys())
    allowed_preview = allowed_outline_hint(allowed_for_llm, limit=30)

    # 3) ê·œì¹™ ê¸°ë°˜(ìš”ëª©í‘œ) í›„ë³´ â€” ê·¼ê±°ìš©ì€ 'ì˜ë¯¸ ìˆëŠ” ë¼ë²¨'ì„ ìš°ì„  í™œìš©
    outline_rank = score_outline_candidates(info, allowed_for_llm)

    # 4) LLM: í—ˆìš© 3ìë¦¬ ê°•ì œ(ì •ì œëœ ëª©ë¡ ì‚¬ìš©)
    llm_raw = ask_llm_for_kdc_with_allowed(info, api_key=openai_key, model=model,
                                           anchors=anchors, allowed=allowed_for_llm)
    code = enforce_anchor_digits(llm_raw, anchors)

    # 5) ì‚¬í›„ê²€ì¦: í—ˆìš©ëª©ë¡(ì „ì²´ allowed_all ê¸°ì¤€) ìœ„ë°˜ ì‹œ ë³´ì •
    head3 = None
    if code:
        m = re.match(r"^(\d{3})", code)
        if m:
            head3 = m.group(1)
    if code and head3 not in allowed_set_all:
        st.warning(f"LLM ê²°ê³¼({code})ì˜ ê¸°ë³¸ 3ìë¦¬ {head3}ê°€ í—ˆìš© ëª©ë¡ì— ì—†ìŒ â†’ ê·œì¹™ ê¸°ë°˜ ìµœê³  í›„ë³´ë¡œ ë³´ì •")
        if outline_rank:
            best = outline_rank[0]["code"]
            tail = ""
            m2 = re.match(r"^\d{3}(\.[0-9]+)?$", code)
            if m2 and m2.group(1):
                tail = m2.group(1)
            code = best + (tail or "")
            head3 = best
        else:
            fallback = sorted(list(allowed_set_all))[0] if allowed_set_all else None
            code = fallback or code

    # 6) LLM í›„ë³´(ê·¼ê±°í‘œ) ìƒì„±
    ranking = ask_llm_for_kdc_ranking(info, api_key=openai_key, model=model,
                                      anchors=anchors, allowed=allowed_for_llm)

    # 7) ë””ë²„ê·¸ ì…ë ¥
    with st.expander("LLM ì…ë ¥ ì •ë³´(í™•ì¸ìš©)"):
        st.json({
            "title": info.title, "author": info.author, "publisher": info.publisher, "pub_date": info.pub_date,
            "isbn13": info.isbn13, "category": info.category,
            "description": (info.description[:600] + "â€¦") if info.description and len(info.description) > 600 else info.description,
            "toc": info.toc, "ea_add_last3": last3, "anchors": anchors,
            "allowed_size": len(allowed_for_llm), "allowed_preview": allowed_preview,
            "llm_raw": llm_raw, "final_code": code
        })

    signals = {"title": info.title[:120], "category": info.category[:120], "author": info.author[:80], "publisher": info.publisher[:80]}
    return {"code": code, "anchors": anchors, "ea_add_last3": last3, "ranking": ranking,
            "signals": signals, "llm_raw": llm_raw,
            "allowed_size": len(allowed_for_llm), "allowed_preview": allowed_preview,
            "outline_rank": outline_rank}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“š ISBN â†’ KDC ì¶”ì²œ (ì•Œë¼ë”˜ â†’ ìš”ëª©í‘œ(3ìë¦¬) ê°•ì œ + ìë¦¬ì•µì»¤ + ì±—G)")
st.caption("ì•Œë¼ë”˜ ì„œì§€ + EA ìë¦¬ì•µì»¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì±—Gê°€ **ìš”ëª©í‘œ(3ìë¦¬) í—ˆìš© ëª©ë¡ ì•ˆì—ì„œë§Œ** ë¶„ë¥˜ë¥¼ ì •í•©ë‹ˆë‹¤.")

isbn = st.text_input("ISBN-13 ì…ë ¥", placeholder="ì˜ˆ: 9791193904565").strip()
go = st.button("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œ")

if go:
    if not isbn:
        st.warning("ISBNì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        norm = normalize_isbn13(isbn)
        if not norm or len(norm) != 13:
            st.info("ISBN-13 í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        with st.spinner("EA ìë¦¬ì•µì»¤ í™•ì¸ â†’ ì•Œë¼ë”˜ ì •ë³´ ìˆ˜ì§‘ â†’ ìš”ëª©í‘œ í—ˆìš©ëª©ë¡ êµ¬ì„± â†’ ì±—G íŒë‹¨â€¦"):
            result = get_kdc_from_isbn(isbn13=norm or isbn, ttbkey=ALADIN_TTBKEY, openai_key=OPENAI_API_KEY, model=MODEL)

        st.subheader("ê²°ê³¼")
        last3 = result.get("ea_add_last3")
        anchors = result.get("anchors") or {}
        pattern = anchors.get("pattern", "x-x-x")
        if last3:
            st.markdown(f"- **EA_ADD_CODE ë’¤ 3ìë¦¬**: `{last3}` â†’ **ìë¦¬ì•µì»¤ íŒ¨í„´**: `{pattern}`")
        else:
            st.markdown("- **EA_ADD_CODE**: ì¡°íšŒ ì‹¤íŒ¨(ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰)")
        code = result.get("code")
        if code:
            st.markdown(f"### âœ… ì¶”ì²œ KDC: **`{code}`**")
            st.caption("â€» ì±—GëŠ” ìš”ëª©í‘œ(3ìë¦¬) í—ˆìš© ëª©ë¡ ì•ˆì—ì„œë§Œ ì„ íƒí•˜ë„ë¡ ê°•ì œë˜ë©°, ìë¦¬ì•µì»¤ë¡œ ë³´ì •ë©ë‹ˆë‹¤.")
        else:
            st.error("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ISBN/í‚¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¼ê±°/ìˆœìœ„Â·ì¡°í•© + ì„¸ë¶€ ìš”ì†Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        st.markdown("#### ğŸ” ì¶”ì²œ ê·¼ê±° (ìš”ëª©í‘œ í—ˆìš©ëª©ë¡ + ê·œì¹™ ì ì¤‘ + LLM í›„ë³´)")
        st.markdown(f"- **í—ˆìš© 3ìë¦¬ ê°œìˆ˜**: {result.get('allowed_size',0)}")
        st.markdown(f"- **í—ˆìš© 3ìë¦¬ ë¯¸ë¦¬ë³´ê¸°**: {result.get('allowed_preview') or '-'}")

        sig = result.get("signals") or {}
        ranking = result.get("ranking") or []
        llm_raw = result.get("llm_raw")
        outline_rank = result.get("outline_rank") or []

        st.markdown(f"- **EA ìë¦¬ì•µì»¤**: ë°±={anchors.get('hundreds') or 'x'}, ì‹­={anchors.get('tens') or 'x'}, ì¼={anchors.get('units') or 'x'} (íŒ¨í„´ `{pattern}`)")
        st.markdown(f"- **LLM ì›ì¶œë ¥**: `{llm_raw or '-'}` â†’ ì•µì»¤/í—ˆìš©ëª©ë¡ ê²€ì¦ í›„ â†’ `{code or '-'}`")
        st.markdown(f"- **ì‚¬ìš© ë©”íƒ€ë°ì´í„°**: ì œëª©='{sig.get('title','')}', ì¹´í…Œê³ ë¦¬='{sig.get('category','')}', ì €ì='{sig.get('author','')}', ì¶œíŒì‚¬='{sig.get('publisher','')}'")

        import pandas as _pd

        # 1) ìš”ëª©í‘œ ê·œì¹™ í›„ë³´(ë³´ì¡° ê·¼ê±°)
        if outline_rank:
            rows_rb = []
            for i, c in enumerate(outline_rank, start=1):
                rows_rb.append({
                    "ìˆœìœ„(RB)": i,
                    "KDC(3ìë¦¬)": c.get("code"),
                    "ë¼ë²¨": c.get("label",""),
                    "í‚¤ì›Œë“œ ì ì¤‘": ", ".join(c.get("hits",[])[:10]),
                    "ê·œì¹™ ì‹ ë¢°ë„": f"{c.get('conf',0)*100:.1f}%"
                })
            st.markdown("**ìš”ëª©í‘œ(3ìë¦¬) ê¸°ë°˜ ê·œì¹™ í›„ë³´**")
            st.dataframe(_pd.DataFrame(rows_rb), use_container_width=True)
        else:
            st.caption("ìš”ëª©í‘œ ê¸°ë°˜ ê·œì¹™ í›„ë³´: ì ì¤‘ ì—†ìŒ")

        # 2) LLM í›„ë³´(í—ˆìš©ëª©ë¡ ê°•ì œ í•˜ì—ì„œì˜ íŒë‹¨)
        if ranking:
            rows = []
            for i, c in enumerate(ranking, start=1):
                code_i = c.get("code") or ""
                conf = c.get("confidence")
                try: conf_pct = f"{float(conf)*100:.1f}%"
                except: conf_pct = ""
                factors = c.get("factors", {}) if isinstance(c.get("factors"), dict) else {}
                rows.append({
                    "ìˆœìœ„(LLM)": i,
                    "KDC í›„ë³´": code_i,
                    "ì‹ ë¢°ë„": conf_pct,
                    "ê·¼ê±° í‚¤ì›Œë“œ": ", ".join((c.get("evidence_terms") or [])[:8]),
                    "ê°€ì¤‘ì¹˜(title/category/author/publisher/desc/toc)": ", ".join(
                        [f"{k}:{factors.get(k):.2f}" for k in ["title","category","author","publisher","desc","toc"]
                         if isinstance(factors.get(k), (int, float))]
                    ) or "-",
                })
            st.markdown("**LLM ìƒìœ„ í›„ë³´(ìš”ëª©í‘œ í—ˆìš©ëª©ë¡ ê¸°ë°˜)**")
            st.dataframe(_pd.DataFrame(rows), use_container_width=True)
        else:
            st.caption("LLM í›„ë³´: ìƒì„± ì•ˆ ë¨ (JSON ì‹¤íŒ¨/ì •ë³´ ë¶€ì¡±)")

