# new056.py (EA_ADD_CODE â†’ ìë¦¬ë³„ ì•µì»¤ ê³ ì • + ê·¼ê±°/ìˆœìœ„ íŒŒì‹± ë³´ê°•)
# ê¸°ì¡´ UI ìœ ì§€: â‘  EA_ADD_CODE ë’¤ 3ìë¦¬ ì¤‘ 0ì´ ì•„ë‹Œ ìë¦¬ ê³ ì • â†’ â‘¡ ì•Œë¼ë”˜ + LLM â†’ â‘¢ ê·¼ê±°(ìˆœìœ„Â·ì¡°í•© + ì„¸ë¶€ ìš”ì†Œ)

import os
import re
import json
import html
import urllib.parse
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
from bs4 import BeautifulSoup
from pathlib import Path

import requests
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ISBN â†’ KDC ì¶”ì²œ",
    page_icon="ğŸ“š",
    layout="centered"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜/ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "gpt-4o-mini"
ALADIN_LOOKUP_URL = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx"
OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"
NLK_SEARCH_API = "https://www.nl.go.kr/NL/search/openApi/search.do"
NLK_SEOJI_API  = "https://www.nl.go.kr/seoji/SearchApi.do"

with st.expander("í™˜ê²½ì„¤ì • ë””ë²„ê·¸", expanded=True):
    st.write("ğŸ“ ì•± í´ë”:", Path(__file__).resolve().parent.as_posix())
    st.write("ğŸ” secrets.toml ì¡´ì¬?:", (Path(__file__).resolve().parent / ".streamlit" / "secrets.toml").exists())
    st.write("ğŸ”‘ st.secrets í‚¤ë“¤:", list(st.secrets.keys()))
    st.write("api_keys ë‚´ìš©:", dict(st.secrets.get("api_keys", {})))
    st.write("âœ… openai_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("openai_key")))
    st.write("âœ… aladin_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("aladin_key")))
    st.write("âœ… nlk_key ë¡œë“œë¨?:", bool(st.secrets.get("api_keys", {}).get("nlk_key")))

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; KDCFetcher/1.0; +https://example.local)"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ secrets.toml ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_secret(*path, default=""):
    try:
        v = st.secrets
        for p in path:
            v = v[p]
        return v
    except Exception:
        return default

OPENAI_API_KEY = (_get_secret("api_keys", "openai_key") or os.environ.get("OPENAI_API_KEY", ""))
ALADIN_TTBKEY  = (_get_secret("api_keys", "aladin_key")  or os.environ.get("ALADIN_TTBKEY", ""))
NLK_API_KEY    = (_get_secret("api_keys", "nlk_key")     or os.environ.get("NLK_API_KEY", ""))
MODEL = DEFAULT_MODEL

@dataclass
class BookInfo:
    title: str = ""
    author: str = ""
    pub_date: str = ""
    publisher: str = ""
    isbn13: str = ""
    category: str = ""
    description: str = ""
    toc: str = ""
    extra: Dict[str, Any] = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def first_match_number(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"\b([0-9]{1,3}(?:\.[0-9]+)?)\b", text)
    return m.group(1) if m else None

def strip_tags(html_text: str) -> str:
    return re.sub(r"<[^>]+>", " ", html_text)

def normalize_isbn13(isbn: str) -> str:
    s = re.sub(r"[^0-9Xx]", "", isbn or "")
    return s[-13:] if len(s) >= 13 else s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EA_ADD_CODE ì¡°íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_ea_add_code_last3(isbn13: str, key: str) -> Optional[str]:
    """1ì°¨: ì„œì§€ API â†’ docs[0].EA_ADD_CODE, 2ì°¨: ì¼ë°˜ê²€ìƒ‰ API â†’ recordList[0].EA_ADD_CODE"""
    if not key:
        st.info("NLK_API_KEYê°€ ì—†ì–´ EA_ADD_CODE ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    # 1) ì„œì§€(ISBN) API ì‹œë„
    try:
        p1 = {"cert_key": key, "result_style": "json", "page_no": 1, "page_size": 5, "isbn": isbn13}
        r1 = requests.get(NLK_SEOJI_API, params=p1, headers=HEADERS, timeout=10)
        r1.raise_for_status()
        d1 = r1.json()
        docs = d1.get("docs") if isinstance(d1, dict) else None
        if isinstance(docs, list) and docs:
            ea = docs[0].get("EA_ADD_CODE") or docs[0].get("ea_add_code")
            if ea:
                m = re.search(r"(\d{3})$", str(ea))
                if m:
                    last3 = m.group(1)
                    st.success(f"(ì„œì§€API) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                    return last3
    except Exception as e:
        st.info(f"ì„œì§€API ì‹¤íŒ¨ â†’ ì¼ë°˜ê²€ìƒ‰ ë°±ì—…: {e}")
    # 2) ì¼ë°˜ê²€ìƒ‰ ë°±ì—…
    try:
        p2 = {"key": key, "srchTarget": "total", "kwd": isbn13, "pageNum": 1, "pageSize": 1, "apiType": "json"}
        r2 = requests.get(NLK_SEARCH_API, params=p2, headers=HEADERS, timeout=10)
        r2.raise_for_status()
        d2 = r2.json()
        result = d2.get("result") if isinstance(d2, dict) else None
        if isinstance(result, list):
            result = result[0] if result else {}
        recs = None
        if isinstance(result, dict):
            recs = result.get("recordList") or result.get("recordlist") or result.get("records") or result.get("record")
        if isinstance(recs, dict):
            recs = [recs]
        if isinstance(recs, list) and recs:
            rec0 = recs[0]
            if isinstance(rec0, list) and rec0:
                rec0 = rec0[0]
            if isinstance(rec0, dict):
                ea = rec0.get("EA_ADD_CODE") or rec0.get("ea_add_code")
                if ea:
                    m = re.search(r"(\d{3})$", str(ea))
                    if m:
                        last3 = m.group(1)
                        st.success(f"(ì¼ë°˜ê²€ìƒ‰) EA_ADD_CODE: {ea} â†’ ë’¤ 3ìë¦¬={last3}")
                        return last3
        st.warning("NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: ì‘ë‹µ êµ¬ì¡° ë¯¸ì¼ì¹˜")
        return None
    except Exception as e:
        st.warning(f"NLK SearchApi EA_ADD_CODE ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìë¦¬ë³„ ì•µì»¤ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_anchor_from_last3(last3: Optional[str]) -> Dict[str, Optional[str]]:
    """
    last3 ì˜ˆ: '813' â†’ ë°±=8, ì‹­=1, ì¼=3 (0ë³´ë‹¤ í° ìë¦¬ë§Œ ê³ ì •)
    '800' â†’ ë°±=8ë§Œ ê³ ì •, 'x-x-x' íŒ¨í„´ í‘œê¸°
    """
    anchors = {"hundreds": None, "tens": None, "units": None, "pattern": "x-x-x"}
    if not (last3 and len(last3) == 3 and last3.isdigit()):
        return anchors
    h, t, u = last3[0], last3[1], last3[2]
    anchors["hundreds"] = h if int(h) > 0 else None
    anchors["tens"]     = t if int(t) > 0 else None
    anchors["units"]    = u if int(u) > 0 else None
    anchors["pattern"]  = f"{anchors['hundreds'] or 'x'}-{anchors['tens'] or 'x'}-{anchors['units'] or 'x'}"
    return anchors

def anchor_clause_for_prompt(anc: Dict[str, Optional[str]]) -> str:
    rules = []
    if anc.get("hundreds"): rules.append(f"ë°±ì˜ ìë¦¬ëŠ” {anc['hundreds']}")
    if anc.get("tens"):     rules.append(f"ì‹­ì˜ ìë¦¬ëŠ” {anc['tens']}")
    if anc.get("units"):    rules.append(f"ì¼ì˜ ìë¦¬ëŠ” {anc['units']}")
    if not rules:
        return ""
    mask = anc.get("pattern", "x-x-x").replace("-", "")
    examples = [mask.replace("x", d) for d in ["0","1","2"]]
    return (
        " ë°˜ë“œì‹œ ë‹¤ìŒ ìë¦¿ìˆ˜ ì œì•½ì„ ì§€ì¼œë¼: " + ", ".join(rules) +
        f". ì¦‰, ë¶„ë¥˜ë²ˆí˜¸ëŠ” '{mask}' íŒ¨í„´ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•œë‹¤(ì˜ˆ: {', '.join(e + '.7' for e in examples)}). "
    )

def enforce_anchor_digits(code: Optional[str], anc: Dict[str, Optional[str]]) -> Optional[str]:
    if not code:
        return code
    m = re.match(r"^(\d{1,3})(.*)$", code)
    if not m:
        return code
    head, tail = m.group(1), m.group(2)
    head = (head + "000")[:3]  # 3ìë¦¬ ë³´ì •
    h, t, u = list(head)
    if anc.get("hundreds"): h = anc["hundreds"]
    if anc.get("tens"):     t = anc["tens"]
    if anc.get("units"):    u = anc["units"]
    fixed = f"{h}{t}{u}" + tail
    return fixed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•Œë¼ë”˜ API/ì›¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def aladin_lookup_by_api(isbn13: str, ttbkey: str) -> Optional[BookInfo]:
    if not ttbkey:
        return None
    params = {
        "ttbkey": ttbkey, "itemIdType": "ISBN13", "ItemId": isbn13,
        "output": "js", "Version": "20131101",
        "OptResult": "authors,categoryName,fulldescription,toc,packaging,ratings"
    }
    try:
        r = requests.get(ALADIN_LOOKUP_URL, params=params, headers=HEADERS, timeout=15)
        r.raise_for_status()
        data = r.json()
        items = data.get("item", [])
        if not items:
            st.info("ì•Œë¼ë”˜ API(ItemLookUp)ì—ì„œ ê²°ê³¼ ì—†ìŒ â†’ ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
            return None
        it = items[0]
        return BookInfo(
            title=clean_text(it.get("title")), author=clean_text(it.get("author")),
            pub_date=clean_text(it.get("pubDate")), publisher=clean_text(it.get("publisher")),
            isbn13=clean_text(it.get("isbn13")) or isbn13, category=clean_text(it.get("categoryName")),
            description=clean_text(it.get("fulldescription")) or clean_text(it.get("description")),
            toc=clean_text(it.get("toc")), extra=it,
        )
    except Exception as e:
        st.info(f"ì•Œë¼ë”˜ API í˜¸ì¶œ ì˜ˆì™¸ â†’ {e} / ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
        return None

def aladin_lookup_by_web(isbn13: str) -> Optional[BookInfo]:
    try:
        params = {"SearchTarget": "Book", "SearchWord": f"isbn:{isbn13}"}
        sr = requests.get(ALADIN_SEARCH_URL, params=params, headers=HEADERS, timeout=15)
        sr.raise_for_status()
        soup = BeautifulSoup(sr.text, "html.parser")
        link_tag = soup.select_one("a.bo3")
        item_url = None
        if link_tag and link_tag.get("href"):
            item_url = urllib.parse.urljoin("https://www.aladin.co.kr", link_tag["href"])
        if not item_url:
            m = re.search(r'href=[\'\"](/shop/wproduct\.aspx\?ItemId=\d+[^\'\"]*)[\'\"]', sr.text, re.I)
            if m: item_url = urllib.parse.urljoin("https://www.aladin.co.kr", html.unescape(m.group(1)))
        if not item_url:
            first_card = soup.select_one(".ss_book_box, .ss_book_list")
            if first_card:
                a = first_card.find("a", href=True)
                if a: item_url = urllib.parse.urljoin("https://www.aladin.co.kr", a["href"])
        if not item_url:
            st.warning("ì•Œë¼ë”˜ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            with st.expander("ë””ë²„ê·¸: ê²€ìƒ‰ í˜ì´ì§€ HTML ì¼ë¶€"):
                st.code(sr.text[:2000])
            return None
        pr = requests.get(item_url, headers=HEADERS, timeout=15)
        pr.raise_for_status()
        psoup = BeautifulSoup(pr.text, "html.parser")
        og_title = psoup.select_one('meta[property="og:title"]')
        og_desc  = psoup.select_one('meta[property="og:description"]')
        title = clean_text(og_title["content"]) if og_title and og_title.has_attr("content") else ""
        desc  = clean_text(og_desc["content"]) if og_desc and og_desc.has_attr("content") else ""
        body_text = clean_text(psoup.get_text(" "))[:4000]
        description = desc or body_text
        author = publisher = pub_date = cat_text = ""
        info_box = psoup.select_one("#Ere_prod_allwrap, #Ere_prod_mconts_wrap, #Ere_prod_titlewrap")
        if info_box:
            text = clean_text(info_box.get_text(" "))
            m_author = re.search(r"(ì €ì|ì§€ì€ì´)\s*:\s*([^\|Â·/]+)", text)
            m_publisher = re.search(r"(ì¶œíŒì‚¬)\s*:\s*([^\|Â·/]+)", text)
            m_pubdate = re.search(r"(ì¶œê°„ì¼|ì¶œíŒì¼)\s*:\s*([0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2})", text)
            if m_author:   author   = clean_text(m_author.group(2))
            if m_publisher: publisher = clean_text(m_publisher.group(2))
            if m_pubdate:  pub_date = clean_text(m_pubdate.group(2))
        crumbs = psoup.select(".location, .path, .breadcrumb")
        if crumbs: cat_text = clean_text(" > ".join(c.get_text(" ") for c in crumbs))
        with st.expander("ë””ë²„ê·¸: ìŠ¤í¬ë ˆì´í•‘ ì§„ì… URL / íŒŒì‹± ê²°ê³¼"):
            st.write({"item_url": item_url, "title": title})
        return BookInfo(title=title, description=description, isbn13=isbn13,
                        author=author, publisher=publisher, pub_date=pub_date, category=cat_text)
    except Exception as e:
        st.error(f"ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì˜ˆì™¸: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc(book: BookInfo, api_key: str, model: str, anchor_clause: str) -> Optional[str]:
    if not api_key:
        raise RuntimeError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°” ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ë„ì„œ ì •ë³´ë¥¼ ë³´ê³  KDC ë¶„ë¥˜ê¸°í˜¸ë¥¼ 'ìˆ«ìë§Œ' ì¶œë ¥í•´ë¼. "
        "í˜•ì‹ ì˜ˆì‹œ: 813.7 / 325.1 / 005 / 181 ë“±. ì„¤ëª…, ì ‘ë‘/ì ‘ë¯¸ í…ìŠ¤íŠ¸, ê¸°íƒ€ ë¬¸ìëŠ” ê¸ˆì§€."
        + anchor_clause
    )
    payload = {
        "title": book.title, "author": book.author, "publisher": book.publisher, "pub_date": book.pub_date,
        "isbn13": book.isbn13, "category": book.category,
        "description": book.description[:1200], "toc": book.toc[:800]
    }
    user_prompt = "ë„ì„œ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) + "\n\nKDC ìˆ«ìë§Œ ì¶œë ¥:"
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.0,
                "max_tokens": 16,
            },
            timeout=30,
        )
        resp.raise_for_status()
        data = resp.json()
        text = (data["choices"][0]["message"]["content"] or "").strip()
        return first_match_number(text)
    except Exception as e:
        st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ JSON íŒŒì‹± ë³´ê°• ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_json_object(text: str) -> Optional[str]:
    """ì‘ë‹µì—ì„œ ìµœìƒìœ„ JSON ê°ì²´ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ(ì½”ë“œíœìŠ¤/í”„ë¦¬í…ìŠ¤íŠ¸ ì„ì„ ëŒ€ë¹„)."""
    if not text:
        return None
    m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", text, re.I)
    if m:
        return m.group(1)
    start = text.find("{")
    if start == -1:
        return None
    depth = 0
    in_str = False
    esc = False
    for i in range(start, len(text)):
        ch = text[i]
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return text[start:i+1]
    return None

def _sanitize_json(s: str) -> str:
    """ìì˜í•œ ë¬¸ë²• ì˜¤ë¥˜ ë³´ì •: ìŠ¤ë§ˆíŠ¸ì¿¼íŠ¸/íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ/ì»¨íŠ¸ë¡¤ë¬¸ì ì œê±° ë“±."""
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = re.sub(r"```.*?```", "", s, flags=re.S)
    s = re.sub(r",\s*([}\]])", r"\1", s)
    s = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f]", "", s)
    return s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¼ê±°/ìˆœìœ„ JSON íŒŒì‹± ë³´ê°• â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc_ranking(book: BookInfo, api_key: str, model: str, anchor_clause: str) -> Optional[List[Dict[str, Any]]]:
    if not api_key:
        return None
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ë„ì„œ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ìƒìœ„ í›„ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ë¼. "
        'ë°˜ë“œì‹œ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ì§€ì¼œë¼: {"candidates":[{"code":str,"confidence":number,'
        '"evidence_terms":[str...],"_view":str,"factors":{"title":number,"category":number,'
        '"author":number,"publisher":number,"desc":number,"toc":number}}]} '
        "ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€. ì½”ë“œíœìŠ¤ ê¸ˆì§€. ë°°ì—´ ê¸¸ì´ëŠ” 3~5. "
        + anchor_clause
    )
    payload = {
        "title": book.title, "author": book.author, "publisher": book.publisher,
        "pub_date": book.pub_date, "isbn13": book.isbn13, "category": book.category,
        "description": book.description[:1200], "toc": book.toc[:800]
    }
    user_prompt = (
        "ë„ì„œ ì •ë³´(JSON):\n" + json.dumps(payload, ensure_ascii=False, indent=2) +
        "\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒìœ„ í›„ë³´ 3~5ê°œë¥¼ confidence ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì‚°ì¶œí•´, "
        "ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ ë°˜í™˜í•´."
    )
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.0,
                "max_tokens": 520,
            },
            timeout=30,
        )
        resp.raise_for_status()
        text = (resp.json()["choices"][0]["message"]["content"] or "").strip()
        raw = _extract_json_object(text) or text
        safe = _sanitize_json(raw)
        parsed = json.loads(safe)

        cands = parsed.get("candidates") if isinstance(parsed, dict) else None
        if isinstance(cands, list) and cands:
            for c in cands:
                if "confidence" in c:
                    try: c["confidence"] = float(c["confidence"])
                    except Exception: pass
                fx = c.get("factors")
                if isinstance(fx, dict):
                    for k, v in list(fx.items()):
                        try: fx[k] = float(v)
                        except Exception: pass
            try:
                cands = sorted(cands, key=lambda x: float(x.get("confidence", 0)), reverse=True)
            except Exception:
                pass
            return cands

        st.info("ê·¼ê±°/ìˆœìœ„ JSON: candidatesê°€ ë¹„ì–´ ìˆê±°ë‚˜ í˜•ì‹ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    except json.JSONDecodeError as je:
        st.warning(f"ê·¼ê±°/ìˆœìœ„ JSON ìƒì„± ì‹¤íŒ¨(JSONDecode): {je}")
        return None
    except Exception as e:
        st.info(f"ê·¼ê±°/ìˆœìœ„ JSON ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê°„ë‹¨ ê·œì¹™ ì ì¤‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_rule_hits(info: BookInfo) -> Dict[str, List[str]]:
    text = f"{info.title} {info.category} {info.description[:400]}" if info else ""
    buckets = {
        "ë¬¸í•™": ["ì†Œì„¤","ë¬¸í•™","ì‹œì§‘","í¬ê³¡","ì—ì„¸ì´","ìˆ˜í•„"],
        "ì‚¬íšŒ": ["ì‚¬íšŒ","ì •ì¹˜","ê²½ì œ","ê²½ì˜","ë²•","í–‰ì •"],
        "ìì—°": ["ê³¼í•™","ìˆ˜í•™","ë¬¼ë¦¬","í™”í•™","ìƒë¬¼","ì§€êµ¬ê³¼í•™"],
        "ê¸°ìˆ ": ["ì˜í•™","ê°„í˜¸","ê³µí•™","ì»´í“¨í„°","í”„ë¡œê·¸ë˜ë°","ì½”ë”©"],
        "ì˜ˆìˆ ": ["ì˜ˆìˆ ","ë¯¸ìˆ ","ë””ìì¸","ìŒì•…","ê±´ì¶•","ì‚¬ì§„"],
        "ì–¸ì–´": ["ì–¸ì–´","ë¬¸ë²•","êµ­ì–´","ì˜ì–´","ì¼ë³¸ì–´","ì¤‘êµ­ì–´"],
        "ì—­ì‚¬": ["ì—­ì‚¬","í•œêµ­ì‚¬","ì„¸ê³„ì‚¬","ê³ ê³ í•™"],
        "ì² í•™": ["ì² í•™","ì‚¬ìƒ","ì¸ë¬¸"],
    }
    hits = {}
    for k, words in buckets.items():
        matched = [w for w in words if w in text]
        if matched:
            hits[k] = matched
    return hits

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_kdc_from_isbn(isbn13: str, ttbkey: Optional[str], openai_key: str, model: str) -> Dict[str, Any]:
    # 0) EA â†’ last3 & ìë¦¬ë³„ ì•µì»¤
    last3 = get_ea_add_code_last3(isbn13, NLK_API_KEY)
    anchors = build_anchor_from_last3(last3)

    # 1) ì•Œë¼ë”˜
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)
    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return {"code": None, "anchors": anchors, "ea_add_last3": last3, "ranking": None, "signals": None, "llm_raw": None}

    # 2) LLM (ìë¦¬ë³„ ì•µì»¤ ì œì•½ ë°˜ì˜)
    anchor_clause = anchor_clause_for_prompt(anchors)
    llm_raw = ask_llm_for_kdc(info, api_key=openai_key, model=model, anchor_clause=anchor_clause)
    ranking  = ask_llm_for_kdc_ranking(info, api_key=openai_key, model=model, anchor_clause=anchor_clause)

    # 3) ìë¦¬ë³„ ì•µì»¤ ê°•ì œ ë³´ì •
    code = enforce_anchor_digits(llm_raw, anchors)

    # 4) ë””ë²„ê·¸ ì…ë ¥
    with st.expander("LLM ì…ë ¥ ì •ë³´(í™•ì¸ìš©)"):
        st.json({
            "title": info.title, "author": info.author, "publisher": info.publisher, "pub_date": info.pub_date,
            "isbn13": info.isbn13, "category": info.category,
            "description": (info.description[:600] + "â€¦") if info.description and len(info.description) > 600 else info.description,
            "toc": info.toc, "ea_add_last3": last3, "anchors": anchors,
            "anchor_clause": anchor_clause, "llm_raw": llm_raw,
        })

    # 5) ì‹ í˜¸ ìš”ì•½ + ê·œì¹™ ì ì¤‘ ë‚´ì—­
    signals = {"title": info.title[:120], "category": info.category[:120], "author": info.author[:80], "publisher": info.publisher[:80]}
    rule_hits = extract_rule_hits(info)

    return {"code": code, "anchors": anchors, "ea_add_last3": last3, "ranking": ranking, "signals": signals, "llm_raw": llm_raw, "rule_hits": rule_hits}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“š ISBN â†’ KDC ì¶”ì²œ (EA ìë¦¬ì•µì»¤ + ì•Œë¼ë”˜ + ì±—G)")
st.caption("â‘  EA_ADD_CODE ë’¤ 3ìë¦¬ì—ì„œ 0ì´ ì•„ë‹Œ ìë¦¬ ìˆ«ìë¥¼ ê³ ì •(ì˜ˆ: 813â†’8Â·1Â·3 ê³ ì •, 800â†’8ë§Œ ê³ ì •) â†’ â‘¡ ì•Œë¼ë”˜ ìˆ˜ì§‘ â†’ â‘¢ ì±—Gë¡œ KDC ë„ì¶œ")

isbn = st.text_input("ISBN-13 ì…ë ¥", placeholder="ì˜ˆ: 9791193904565").strip()
go = st.button("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œ")

if go:
    if not isbn:
        st.warning("ISBNì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        norm = normalize_isbn13(isbn)
        if not norm or len(norm) != 13:
            st.info("ISBN-13 í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        with st.spinner("EA ìë¦¬ì•µì»¤ í™•ì¸ â†’ ì•Œë¼ë”˜ ì •ë³´ ìˆ˜ì§‘ â†’ ì±—G íŒë‹¨â€¦"):
            result = get_kdc_from_isbn(isbn13=norm or isbn, ttbkey=ALADIN_TTBKEY, openai_key=OPENAI_API_KEY, model=MODEL)

        st.subheader("ê²°ê³¼")
        last3 = result.get("ea_add_last3")
        anchors = result.get("anchors") or {}
        pattern = anchors.get("pattern", "x-x-x")
        if last3:
            st.markdown(f"- **EA_ADD_CODE ë’¤ 3ìë¦¬**: `{last3}` â†’ **ì•µì»¤ íŒ¨í„´**: `{pattern}`")
        else:
            st.markdown("- **EA_ADD_CODE**: ì¡°íšŒ ì‹¤íŒ¨(ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰)")
        code = result.get("code")
        if code:
            st.markdown(f"### âœ… ì¶”ì²œ KDC: **`{code}`**")
            st.caption("â€» ìë¦¬ì•µì»¤(ë°±/ì‹­/ì¼ì˜ ìë¦¬) ì œì•½ì„ ìš°ì„  ì ìš©í•˜ì—¬ LLM ê²°ê³¼ë¥¼ ë³´ì •í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.error("ë¶„ë¥˜ê¸°í˜¸ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ISBN/í‚¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¼ê±°/ìˆœìœ„Â·ì¡°í•© + ì„¸ë¶€ ìš”ì†Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        st.markdown("#### ğŸ” ì¶”ì²œ ê·¼ê±° (ìˆœìœ„Â·ì¡°í•© + ì„¸ë¶€ ìš”ì†Œ)")
        sig = result.get("signals") or {}
        rule_hits = result.get("rule_hits") or {}
        ranking = result.get("ranking") or []
        llm_raw = result.get("llm_raw")
        st.markdown(f"- **EA ìë¦¬ì•µì»¤**: ë°±={anchors.get('hundreds') or 'x'}, ì‹­={anchors.get('tens') or 'x'}, ì¼={anchors.get('units') or 'x'} (íŒ¨í„´ `{pattern}`)")
        st.markdown(f"- **LLM ì›ì¶œë ¥**: `{llm_raw or '-'}` â†’ ì•µì»¤ ë³´ì • â†’ `{code or '-'}`")
        st.markdown(f"- **ì‚¬ìš© ë©”íƒ€ë°ì´í„°**: ì œëª©='{sig.get('title','')}', ì¹´í…Œê³ ë¦¬='{sig.get('category','')}', ì €ì='{sig.get('author','')}', ì¶œíŒì‚¬='{sig.get('publisher','')}'")
        if rule_hits:
            st.markdown("- **ê·œì¹™ ì ì¤‘**: " + ", ".join([f"{k}â†’{'+'.join(v)}" for k, v in rule_hits.items()]))
        else:
            st.markdown("- **ê·œì¹™ ì ì¤‘**: ì—†ìŒ")

        if ranking:
            import pandas as _pd
            rows = []
            for i, c in enumerate(ranking, start=1):
                code_i = c.get("code"); conf = c.get("confidence")
                try: conf_pct = f"{float(conf)*100:.1f}%"
                except Exception: conf_pct = ""
                factors = c.get("factors", {}) if isinstance(c.get("factors"), dict) else {}
                rows.append({
                    "ìˆœìœ„": i,
                    "KDC í›„ë³´": code_i,
                    "ì‹ ë¢°ë„": conf_pct,
                    "ê·¼ê±° í‚¤ì›Œë“œ": ", ".join((c.get("evidence_terms") or [])[:8]),
                    "ê°€ì¤‘ì¹˜(title/category/author/publisher/desc/toc)": ", ".join(
                        [f"{k}:{factors.get(k):.2f}" for k in ["title","category","author","publisher","desc","toc"]
                         if isinstance(factors.get(k), (int, float))]
                    ) or "-",
                    "ì°¸ì¡° ë·°": c.get("_view", "")
                })
            df = _pd.DataFrame(rows)
            try:
                from caas_jupyter_tools import display_dataframe_to_user as _disp
                _disp("ì¶”ì²œ ê·¼ê±°(ìˆœìœ„í‘œ)", df)
            except Exception:
                st.dataframe(df, use_container_width=True)
        else:
            st.info("ê·¼ê±° í‘œì‹œëŠ” ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (LLM JSON ì‹¤íŒ¨ ë˜ëŠ” ì‹ í˜¸ ë¶€ì¡±)")
